
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Comprehensive educational resource for NSW HSC Software Engineering syllabus">
      
      
      
        <link rel="canonical" href="https://eatham532.github.io/Software-Engineering-HSC-Textbook/Year12/SoftwareEngineeringProject/Chapter-26-Testing-and-evaluating/26-03-Evaluating-the-software-solution/">
      
      
        <link rel="prev" href="../26-02-Feedback-analysis/quiz/">
      
      
        <link rel="next" href="quiz/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>Content - Software Engineering Textbook HSC</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/diagram-modal.css">
    
      <link rel="stylesheet" href="../../../../assets/quiz.css">
    
      <link rel="stylesheet" href="../../../../assets/common.css">
    
      <link rel="stylesheet" href="../../../../assets/diagram-fix.css">
    
      <link rel="stylesheet" href="../../../../assets/cross-reference.css">
    
      <link rel="stylesheet" href="../../../../assets/site-banner.css">
    
      <link rel="stylesheet" href="../../../../assets/code-runner.css">
    
      <link rel="stylesheet" href="../../../../assets/code-editor.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-58275RL1P9"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-58275RL1P9",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-58275RL1P9",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#263-evaluating-the-software-solution" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
  
  
    
    
    
    <div class="site-banner site-banner--warning" data-banner data-banner-version="1" role="status" aria-live="polite">
      <div class="site-banner__inner">
        <span class="site-banner__icon" aria-hidden="true">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            
              <path d="M12 9v4"/><path d="M12 17h.01"/><path d="M10.29 3.86 1.82 18a2 2 0 0 0 1.71 3h16.94a2 2 0 0 0 1.71-3L13.71 3.86a2 2 0 0 0-3.42 0Z"/>
            
          </svg>
        </span>
        <p class="site-banner__text">
          This textbook is in <strong>beta</strong> ‚Äì content is actively being refined.
          
            <a class="site-banner__link" href="https://github.com/Eatham532/Software-Engineering-HSC-Textbook/issues/new/choose" target="_blank" rel="noopener">Report issues or suggestions</a>
          
        </p>
        <button type="button" class="site-banner__close" aria-label="Dismiss banner" title="Dismiss" data-banner-dismiss>&times;</button>
      </div>
    </div>
    <script>
      (function(){
        var banner=document.querySelector('[data-banner]');
        if(!banner) return;
        var version=banner.getAttribute('data-banner-version')||'1';
        var KEY='site_banner_dismissed_v'+version;
        try{ if(localStorage.getItem(KEY)){ banner.remove(); return;} }catch(e){}
        var btn=banner.querySelector('[data-banner-dismiss]');
        if(btn){ btn.addEventListener('click',function(){
          banner.classList.add('is-hiding');
          setTimeout(function(){ banner && banner.remove(); },200);
          try{ localStorage.setItem(KEY,'1'); }catch(e){}
        }); }
      })();
    </script>
  
  
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Software Engineering Textbook HSC" class="md-header__button md-logo" aria-label="Software Engineering Textbook HSC" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Software Engineering Textbook HSC
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Content
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/Eatham532/Software-Engineering-HSC-Textbook/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    Eatham532/Software-Engineering-HSC-Textbook
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    

    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../Year11/ProgrammingFundamentals/" class="md-tabs__link">
          
  
  
  Year 11

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../ProgrammingForTheWeb/" class="md-tabs__link">
          
  
  
  Year 12

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../code-editor/" class="md-tabs__link">
        
  
  
    
  
  Code Editor

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Software Engineering Textbook HSC" class="md-nav__button md-logo" aria-label="Software Engineering Textbook HSC" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Software Engineering Textbook HSC
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Eatham532/Software-Engineering-HSC-Textbook/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    Eatham532/Software-Engineering-HSC-Textbook
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../../../Year11/ProgrammingFundamentals/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Year 11
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Year 12
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Year 12
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../ProgrammingForTheWeb/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Programming For The Web
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../SecureSoftwareArchitecture/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Secure Software Architecture
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../SoftwareAutomation/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Software Automation
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Software Engineering Project
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Software Engineering Project
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../Chapter-23-Identifying-and-defining/23-01-Requirements-and-feasibility/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Chapter 23 ‚Äî Identifying and defining
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../Chapter-24-Research-and-planning/24-01-Waterfall-approach/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Chapter 24 ‚Äî Research and planning
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../Chapter-25-Producing-and-implementing/25-01-Building-the-solution/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Chapter 25 ‚Äî Producing and implementing
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_5" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4_5" id="__nav_3_4_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 26 ‚Äî Testing and evaluating
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4_5">
            <span class="md-nav__icon md-icon"></span>
            Chapter 26 ‚Äî Testing and evaluating
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../26-01-Testing-methodologies-and-optimisation/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    26.1 Testing methodologies and optimisation
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../26-02-Feedback-analysis/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    26.2 Feedback analysis
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_5_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4_5_3" id="__nav_3_4_5_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    26.3 Evaluating the software solution
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_4_5_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4_5_3">
            <span class="md-nav__icon md-icon"></span>
            26.3 Evaluating the software solution
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Content
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Content
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Learning objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#developing-evaluation-criteria-and-evidence-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      Developing evaluation criteria and evidence synthesis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Developing evaluation criteria and evidence synthesis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluation-criteria-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation criteria framework
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-plan-development-and-testing-outcomes" class="md-nav__link">
    <span class="md-ellipsis">
      Test plan development and testing outcomes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Test plan development and testing outcomes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test-plan-integration-with-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Test plan integration with evaluation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="quiz/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quiz
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code-editor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Code Editor
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Learning objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#developing-evaluation-criteria-and-evidence-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      Developing evaluation criteria and evidence synthesis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Developing evaluation criteria and evidence synthesis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluation-criteria-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation criteria framework
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-plan-development-and-testing-outcomes" class="md-nav__link">
    <span class="md-ellipsis">
      Test plan development and testing outcomes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Test plan development and testing outcomes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test-plan-integration-with-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Test plan integration with evaluation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/Eatham532/Software-Engineering-HSC-Textbook/edit/main/docs/Year12/SoftwareEngineeringProject/Chapter-26-Testing-and-evaluating/26-03-Evaluating-the-software-solution/index.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/Eatham532/Software-Engineering-HSC-Textbook/raw/main/docs/Year12/SoftwareEngineeringProject/Chapter-26-Testing-and-evaluating/26-03-Evaluating-the-software-solution/index.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="263-evaluating-the-software-solution">26.3 Evaluating the software solution<a class="headerlink" href="#263-evaluating-the-software-solution" title="Permanent link">&para;</a></h1>
<p><strong>Outcomes</strong>: SE-12-06</p>
<h2 id="learning-objectives">Learning objectives<a class="headerlink" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<p>By the end of this section, you will be able to:</p>
<ul>
<li>
<p>Develop comprehensive evaluation criteria for judging software solution effectiveness</p>
</li>
<li>
<p>Synthesize feedback and evidence from multiple stakeholders into coherent evaluation reports</p>
</li>
<li>
<p>Create and execute test plans that inform solution evaluation through systematic testing</p>
</li>
<li>
<p>Analyze discrepancies between expected and actual outputs to identify improvement areas</p>
</li>
<li>
<p>Reflect on maintainability, deployment readiness, and lessons learned throughout development</p>
</li>
<li>
<p>Use evaluation results to make informed decisions about solution refinement and future development</p>
</li>
</ul>
<hr />
<h2 id="developing-evaluation-criteria-and-evidence-synthesis">Developing evaluation criteria and evidence synthesis<a class="headerlink" href="#developing-evaluation-criteria-and-evidence-synthesis" title="Permanent link">&para;</a></h2>
<p>Effective software solution evaluation requires clear criteria, systematic evidence collection, and structured analysis that considers multiple perspectives and stakeholder needs. The evaluation process transforms subjective feedback and objective test results into actionable insights for solution improvement.</p>
<h3 id="evaluation-criteria-framework">Evaluation criteria framework<a class="headerlink" href="#evaluation-criteria-framework" title="Permanent link">&para;</a></h3>
<p><strong>Evaluation criteria</strong> define the standards by which software solution success will be measured. These criteria should be specific, measurable, and aligned with project requirements and stakeholder expectations.</p>
<p><strong>Key categories for evaluation criteria:</strong></p>
<ul>
<li>
<p><strong>Functional criteria</strong>: Does the solution perform required functions correctly?</p>
</li>
<li>
<p><strong>Non-functional criteria</strong>: Does the solution meet performance, usability, and quality requirements?</p>
</li>
<li>
<p><strong>Stakeholder satisfaction</strong>: Do stakeholders find the solution valuable and usable?</p>
</li>
<li>
<p><strong>Technical quality</strong>: Is the solution well-designed, maintainable, and scalable?</p>
</li>
<li>
<p><strong>Business impact</strong>: Does the solution achieve intended business outcomes?</p>
</li>
</ul>
<div class="diagram-container" data-container-id="kroki-diagram-0"><button class="diagram-expand-btn" onclick="openDiagramModal('kroki-diagram-0')">üîç View Larger</button><div id="kroki-diagram-0" class="diagram-content"><p><svg xmlns="http://www.w3.org/2000/svg" contentStyleType="text/css" data-diagram-type="DESCRIPTION" height="752px" preserveAspectRatio="xMaxYMax meet" style="width:710px;height:752px;background:#FFFFFF;" version="1.1" viewBox="0 0 710 752" width="710px" zoomAndPan="magnify" id="Kroki" data-diagram-id="kroki-svg-0"><defs /><g><g class="cluster" data-entity="Software Solution Evaluation Process" data-source-line="5" data-uid="ent0002" id="cluster_Software Solution Evaluation Process"><path d="M13.5,11 L310.5986,11 A3.75,3.75 0 0 1 313.0986,13.5 L320.0986,33.2969 L694.5,33.2969 A2.5,2.5 0 0 1 697,35.7969 L697,656.15 A2.5,2.5 0 0 1 694.5,658.65 L13.5,658.65 A2.5,2.5 0 0 1 11,656.15 L11,13.5 A2.5,2.5 0 0 1 13.5,11" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><line style="stroke:#000000;stroke-width:1;" x1="11" x2="320.0986" y1="33.2969" y2="33.2969" /><text fill="#000000" font-family="Verdana" font-size="14" font-weight="bold" lengthAdjust="spacing" textLength="296.0986" x="15" y="25.9951">Software Solution Evaluation Process</text></g><g class="entity" data-entity="criteria" data-source-line="6" data-uid="ent0003" id="entity_criteria"><rect fill="#FFFFFF" height="36.2969" rx="2.5" ry="2.5" style="stroke:#000000;stroke-width:1;" width="170.4453" x="147.78" y="63.12" /><text fill="#000000" font-family="Verdana" font-size="14" lengthAdjust="spacing" textLength="150.4453" x="157.78" y="86.1151">Criteria Development</text></g><g class="entity" data-entity="GMN4" data-source-line="7" data-uid="ent0005" id="entity_GMN4"><path d="M353.01,46 L353.01,77.26 L318.31,81.26 L353.01,85.26 L353.01,116.5313 A0,0 0 0 0 353.01,116.5313 L560.9929,116.5313 A0,0 0 0 0 560.9929,116.5313 L560.9929,56 L550.9929,46 L353.01,46 A0,0 0 0 0 353.01,46" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><path d="M550.9929,46 L550.9929,56 L560.9929,56 L550.9929,46" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="159.2754" x="359.01" y="63.0669">Functional requirements</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="186.9829" x="359.01" y="78.1997">Non-functional requirements</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="166.2007" x="359.01" y="93.3325">Stakeholder expectations</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="127.5942" x="359.01" y="108.4653">Business objectives</text></g><g class="entity" data-entity="evidence" data-source-line="9" data-uid="ent0007" id="entity_evidence"><rect fill="#FFFFFF" height="36.2969" rx="2.5" ry="2.5" style="stroke:#000000;stroke-width:1;" width="157.4023" x="43.3" y="194.65" /><text fill="#000000" font-family="Verdana" font-size="14" lengthAdjust="spacing" textLength="137.4023" x="53.3" y="217.6451">Evidence Collection</text></g><g class="entity" data-entity="GMN8" data-source-line="10" data-uid="ent0009" id="entity_GMN8"><path d="M235.81,177.53 L235.81,208.79 L200.9,212.79 L235.81,216.79 L235.81,248.0612 A0,0 0 0 0 235.81,248.0612 L406.183,248.0612 A0,0 0 0 0 406.183,248.0612 L406.183,187.53 L396.183,177.53 L235.81,177.53 A0,0 0 0 0 235.81,177.53" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><path d="M396.183,177.53 L396.183,187.53 L406.183,187.53 L396.183,177.53" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="75.7783" x="241.81" y="194.5969">Test results</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="93.4819" x="241.81" y="209.7297">User feedback</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="135.8398" x="241.81" y="224.8625">Performance metrics</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="149.373" x="241.81" y="239.9953">Stakeholder interviews</text></g><g class="entity" data-entity="analysis" data-source-line="12" data-uid="ent0011" id="entity_analysis"><rect fill="#FFFFFF" height="36.2969" rx="2.5" ry="2.5" style="stroke:#000000;stroke-width:1;" width="164.9971" x="26.5" y="326.18" /><text fill="#000000" font-family="Verdana" font-size="14" lengthAdjust="spacing" textLength="144.9971" x="36.5" y="349.1751">Analysis &amp; Synthesis</text></g><g class="entity" data-entity="GMN12" data-source-line="13" data-uid="ent0013" id="entity_GMN12"><path d="M226.97,309.06 L226.97,340.33 L191.93,344.33 L226.97,348.33 L226.97,379.5912 A0,0 0 0 0 226.97,379.5912 L377.0305,379.5912 A0,0 0 0 0 377.0305,379.5912 L377.0305,319.06 L367.0305,309.06 L226.97,309.06 A0,0 0 0 0 226.97,309.06" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><path d="M367.0305,309.06 L367.0305,319.06 L377.0305,319.06 L367.0305,309.06" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="83.0591" x="232.97" y="326.1269">Gap analysis</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="126.9214" x="232.97" y="341.2597">Trend identification</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="129.0605" x="232.97" y="356.3925">Root cause analysis</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="126.147" x="232.97" y="371.5253">Impact assessment</text></g><g class="entity" data-entity="reporting" data-source-line="15" data-uid="ent0015" id="entity_reporting"><rect fill="#FFFFFF" height="36.2969" rx="2.5" ry="2.5" style="stroke:#000000;stroke-width:1;" width="239.4473" x="27.28" y="457.71" /><text fill="#000000" font-family="Verdana" font-size="14" lengthAdjust="spacing" textLength="219.4473" x="37.28" y="480.7051">Reporting &amp; Recommendations</text></g><g class="entity" data-entity="GMN16" data-source-line="16" data-uid="ent0017" id="entity_GMN16"><path d="M301.92,440.59 L301.92,471.86 L266.94,475.86 L301.92,479.86 L301.92,511.1213 A0,0 0 0 0 301.92,511.1213 L452.0885,511.1213 A0,0 0 0 0 452.0885,511.1213 L452.0885,450.59 L442.0885,440.59 L301.92,440.59 A0,0 0 0 0 301.92,440.59" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><path d="M442.0885,440.59 L442.0885,450.59 L452.0885,450.59 L442.0885,440.59" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="129.1685" x="307.92" y="457.6569">Executive summary</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="110.2334" x="307.92" y="472.7897">Detailed findings</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="121.5894" x="307.92" y="487.9225">Recommendations</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="69.5386" x="307.92" y="503.0553">Next steps</text></g><g class="entity" data-entity="reflection" data-source-line="18" data-uid="ent0019" id="entity_reflection"><rect fill="#FFFFFF" height="36.2969" rx="2.5" ry="2.5" style="stroke:#000000;stroke-width:1;" width="171.375" x="183.31" y="589.24" /><text fill="#000000" font-family="Verdana" font-size="14" lengthAdjust="spacing" textLength="151.375" x="193.31" y="612.2351">Reflection &amp; Learning</text></g><g class="entity" data-entity="GMN20" data-source-line="19" data-uid="ent0021" id="entity_GMN20"><path d="M389.38,572.12 L389.38,642.6513 L558.6295,642.6513 L558.6295,582.12 L548.6295,572.12 L389.38,572.12" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><path d="M548.6295,572.12 L548.6295,582.12 L558.6295,582.12 L548.6295,572.12" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="105.2949" x="395.38" y="589.1869">Lessons learned</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="148.2495" x="395.38" y="604.3197">Process improvements</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="92.2568" x="395.38" y="619.4525">Best practices</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="141.4893" x="395.38" y="634.5853">Future considerations</text></g><g class="entity" data-entity="GMN28" data-source-line="29" data-uid="ent0029" id="entity_GMN28"><path d="M313.4,705.65 L313.4,745.9156 L634.5997,745.9156 L634.5997,715.65 L624.5997,705.65 L313.4,705.65" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><path d="M624.5997,705.65 L624.5997,715.65 L634.5997,715.65 L624.5997,705.65" fill="#FFFFFF" style="stroke:#000000;stroke-width:1;" /><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="300.1997" x="319.4" y="722.7169">Evaluation is an iterative process that informs</text><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="218.7529" x="319.4" y="737.8497">continuous solution improvement</text></g><g class="link" data-entity-1="reflection" data-entity-2="GMN20" data-source-line="19" data-uid="lnk22" id="link_reflection_GMN20"><path d="M355.08,607.39 C366.39,607.39 377.69,607.39 388.99,607.39" fill="none" id="reflection-GMN20" style="stroke:#000000;stroke-width:1;stroke-dasharray:7.0,7.0;" /></g><g class="link" data-entity-1="criteria" data-entity-2="evidence" data-source-line="21" data-uid="lnk23" id="link_criteria_evidence"><path d="M218.03,99.73 C196.84,124.45 162.244,164.8338 140.994,189.6338" fill="none" id="criteria-to-evidence" style="stroke:#000000;stroke-width:1;" /><polygon fill="#000000" points="137.09,194.19,145.9834,189.9584,140.3433,190.3932,139.9085,184.7531,137.09,194.19" style="stroke:#000000;stroke-width:1;" /></g><g class="link" data-entity-1="evidence" data-entity-2="analysis" data-source-line="22" data-uid="lnk24" id="link_evidence_analysis"><path d="M120.25,231.26 C117.77,255.99 113.8594,294.95 111.3694,319.75" fill="none" id="evidence-to-analysis" style="stroke:#000000;stroke-width:1;" /><polygon fill="#000000" points="110.77,325.72,115.6491,317.1646,111.2695,320.745,107.6891,316.3654,110.77,325.72" style="stroke:#000000;stroke-width:1;" /></g><g class="link" data-entity-1="analysis" data-entity-2="reporting" data-source-line="23" data-uid="lnk25" id="link_analysis_reporting"><path d="M114.12,362.8 C121.38,387.52 132.8715,426.7025 140.1415,451.4925" fill="none" id="analysis-to-reporting" style="stroke:#000000;stroke-width:1;" /><polygon fill="#000000" points="141.83,457.25,143.1356,447.4881,140.4229,452.4521,135.4589,449.7394,141.83,457.25" style="stroke:#000000;stroke-width:1;" /></g><g class="link" data-entity-1="reporting" data-entity-2="reflection" data-source-line="24" data-uid="lnk26" id="link_reporting_reflection"><path d="M163.45,494.33 C186.74,519.05 224.9452,559.6233 248.3052,584.4133" fill="none" id="reporting-to-reflection" style="stroke:#000000;stroke-width:1;" /><polygon fill="#000000" points="252.42,588.78,249.1589,579.4867,248.991,585.1411,243.3366,584.9731,252.42,588.78" style="stroke:#000000;stroke-width:1;" /></g><g class="link" data-entity-1="reflection" data-entity-2="criteria" data-source-line="26" data-uid="lnk27" id="link_reflection_criteria"><path d="M321.17,588.86 C374.87,569.65 453.02,537.96 470,511.12 C486.76,484.63 472.71,471.82 470,440.59 C459.74,322.35 494.94,272.69 424,177.53 C405.51,152.72 335.439,121.9479 285.079,102.0179" fill="none" id="reflection-to-criteria" style="stroke:#000000;stroke-width:1;" /><polygon fill="#000000" points="279.5,99.81,286.3966,106.8412,284.1492,101.6499,289.3404,99.4025,279.5,99.81" style="stroke:#000000;stroke-width:1;" /><text fill="#000000" font-family="Verdana" font-size="13" lengthAdjust="spacing" textLength="232.6162" x="470" y="348.8969">Iterate for continuous improvement</text></g><g class="link" data-entity-1="GMN20" data-entity-2="GMN28" data-source-line="29" data-uid="lnk30" id="link_GMN20_GMN28"><path d="M474,643 C474,663.37 474,688.38 474,705.35" fill="none" id="GMN20-GMN28" style="stroke:#000000;stroke-width:1;stroke-dasharray:7.0,7.0;" /></g></g></svg></p></div></div>
<div class="highlight python-template" data-fence-type="template" data-language="python">
<pre><code class="language-python">class SolutionEvaluator:
    &quot;&quot;&quot;
    Framework for systematically evaluating software solution effectiveness.
    Supports criteria development, evidence synthesis, and comprehensive reporting.
    &quot;&quot;&quot;

    def __init__(self, solution_name, evaluation_scope):
        self.solution_name = solution_name
        self.evaluation_scope = evaluation_scope
        self.evaluation_criteria = {}
        self.evidence_sources = {}
        self.test_results = {}
        self.stakeholder_feedback = {}
        self.evaluation_findings = {}
        self.recommendations = []

    def define_evaluation_criteria(self, functional_criteria, non_functional_criteria, 
                                 stakeholder_criteria, technical_criteria, business_criteria):
        &quot;&quot;&quot;
        Define comprehensive criteria for evaluating solution effectiveness.

        Args:
            functional_criteria: Criteria for functional requirement compliance
            non_functional_criteria: Performance, usability, security criteria
            stakeholder_criteria: User satisfaction and adoption criteria
            technical_criteria: Code quality, maintainability, architecture criteria
            business_criteria: ROI, efficiency, strategic alignment criteria
        &quot;&quot;&quot;
        self.evaluation_criteria = {
            &#x27;functional&#x27;: self._structure_criteria(functional_criteria, &#x27;functional&#x27;),
            &#x27;non_functional&#x27;: self._structure_criteria(non_functional_criteria, &#x27;non_functional&#x27;),
            &#x27;stakeholder&#x27;: self._structure_criteria(stakeholder_criteria, &#x27;stakeholder&#x27;),
            &#x27;technical&#x27;: self._structure_criteria(technical_criteria, &#x27;technical&#x27;),
            &#x27;business&#x27;: self._structure_criteria(business_criteria, &#x27;business&#x27;)
        }

        # Generate measurement approaches for each criterion
        for category, criteria_list in self.evaluation_criteria.items():
            for criterion in criteria_list:
                criterion[&#x27;measurement_approach&#x27;] = self._determine_measurement_approach(
                    criterion[&#x27;description&#x27;], category
                )

    def _structure_criteria(self, criteria_list, category):
        &quot;&quot;&quot;Structure criteria with measurement specifications.&quot;&quot;&quot;
        structured_criteria = []

        for i, criterion_desc in enumerate(criteria_list):
            criterion = {
                &#x27;id&#x27;: f&quot;{category}_{i+1:02d}&quot;,
                &#x27;description&#x27;: criterion_desc,
                &#x27;category&#x27;: category,
                &#x27;weight&#x27;: self._determine_criterion_weight(criterion_desc, category),
                &#x27;target_value&#x27;: self._determine_target_value(criterion_desc),
                &#x27;measurement_method&#x27;: &#x27;TBD&#x27;,
                &#x27;actual_value&#x27;: None,
                &#x27;evaluation_status&#x27;: &#x27;pending&#x27;
            }
            structured_criteria.append(criterion)

        return structured_criteria

    def _determine_criterion_weight(self, criterion_desc, category):
        &quot;&quot;&quot;Determine relative importance weight for criterion.&quot;&quot;&quot;
        # High-priority keywords that increase weight
        high_priority_keywords = [&#x27;security&#x27;, &#x27;performance&#x27;, &#x27;reliability&#x27;, &#x27;usability&#x27;, &#x27;critical&#x27;]

        base_weights = {
            &#x27;functional&#x27;: 0.25,
            &#x27;non_functional&#x27;: 0.20,
            &#x27;stakeholder&#x27;: 0.20,
            &#x27;technical&#x27;: 0.15,
            &#x27;business&#x27;: 0.20
        }

        base_weight = base_weights.get(category, 0.15)

        # Increase weight if criterion contains high-priority keywords
        if any(keyword in criterion_desc.lower() for keyword in high_priority_keywords):
            return min(base_weight * 1.5, 1.0)

        return base_weight

    def _determine_target_value(self, criterion_desc):
        &quot;&quot;&quot;Determine target value based on criterion description.&quot;&quot;&quot;
        # Extract numeric targets from criterion descriptions
        import re

        # Look for percentage targets
        percentage_match = re.search(r&#x27;(\d+)%&#x27;, criterion_desc)
        if percentage_match:
            return f&quot;{percentage_match.group(1)}%&quot;

        # Look for time-based targets
        time_match = re.search(r&#x27;(\d+)\s*(second|minute|hour)&#x27;, criterion_desc)
        if time_match:
            return f&quot;{time_match.group(1)} {time_match.group(2)}(s)&quot;

        # Look for error rate targets
        if &#x27;error&#x27; in criterion_desc.lower():
            return &quot;&lt; 5% error rate&quot;

        # Default qualitative target
        return &quot;Meets expectations&quot;

    def _determine_measurement_approach(self, criterion_desc, category):
        &quot;&quot;&quot;Determine how to measure each criterion.&quot;&quot;&quot;
        measurement_approaches = {
            &#x27;functional&#x27;: &#x27;Test case execution and requirement verification&#x27;,
            &#x27;non_functional&#x27;: &#x27;Performance testing and metric collection&#x27;,
            &#x27;stakeholder&#x27;: &#x27;Surveys, interviews, and usage analytics&#x27;,
            &#x27;technical&#x27;: &#x27;Code analysis, architecture review, and quality metrics&#x27;,
            &#x27;business&#x27;: &#x27;KPI tracking, ROI analysis, and business metric measurement&#x27;
        }

        base_approach = measurement_approaches.get(category, &#x27;Manual evaluation&#x27;)

        # Customize based on criterion content
        if &#x27;performance&#x27; in criterion_desc.lower():
            return &#x27;Automated performance testing with load simulation&#x27;
        elif &#x27;usability&#x27; in criterion_desc.lower():
            return &#x27;User testing sessions and usability metrics&#x27;
        elif &#x27;security&#x27; in criterion_desc.lower():
            return &#x27;Security testing and vulnerability assessment&#x27;
        elif &#x27;accuracy&#x27; in criterion_desc.lower():
            return &#x27;Test data validation and output verification&#x27;

        return base_approach

    def collect_evidence_from_testing(self, test_plan_results, test_coverage_data, 
                                    performance_metrics, defect_tracking_data):
        &quot;&quot;&quot;
        Collect and organize evidence from systematic testing activities.

        Args:
            test_plan_results: Results from executed test plans
            test_coverage_data: Code/requirement coverage information
            performance_metrics: Performance testing measurements
            defect_tracking_data: Bug reports and resolution information
        &quot;&quot;&quot;
        testing_evidence = {
            &#x27;test_execution&#x27;: {
                &#x27;total_tests&#x27;: test_plan_results.get(&#x27;total_tests&#x27;, 0),
                &#x27;passed_tests&#x27;: test_plan_results.get(&#x27;passed_tests&#x27;, 0),
                &#x27;failed_tests&#x27;: test_plan_results.get(&#x27;failed_tests&#x27;, 0),
                &#x27;test_coverage&#x27;: test_coverage_data.get(&#x27;percentage&#x27;, 0),
                &#x27;execution_summary&#x27;: self._analyze_test_execution(test_plan_results)
            },
            &#x27;performance_analysis&#x27;: {
                &#x27;response_times&#x27;: performance_metrics.get(&#x27;response_times&#x27;, {}),
                &#x27;throughput&#x27;: performance_metrics.get(&#x27;throughput&#x27;, {}),
                &#x27;resource_usage&#x27;: performance_metrics.get(&#x27;resource_usage&#x27;, {}),
                &#x27;scalability_results&#x27;: performance_metrics.get(&#x27;scalability&#x27;, {}),
                &#x27;performance_summary&#x27;: self._analyze_performance_metrics(performance_metrics)
            },
            &#x27;quality_metrics&#x27;: {
                &#x27;defect_density&#x27;: defect_tracking_data.get(&#x27;defects_per_kloc&#x27;, 0),
                &#x27;defect_resolution_time&#x27;: defect_tracking_data.get(&#x27;avg_resolution_time&#x27;, 0),
                &#x27;critical_defects&#x27;: defect_tracking_data.get(&#x27;critical_count&#x27;, 0),
                &#x27;defect_trend&#x27;: self._analyze_defect_trends(defect_tracking_data)
            }
        }

        self.evidence_sources[&#x27;testing&#x27;] = testing_evidence

        # Map testing evidence to evaluation criteria
        self._map_evidence_to_criteria(&#x27;testing&#x27;, testing_evidence)

    def collect_stakeholder_feedback(self, user_surveys, stakeholder_interviews, 
                                   usage_analytics, support_tickets):
        &quot;&quot;&quot;
        Collect and organize feedback from various stakeholder sources.

        Args:
            user_surveys: Survey responses from end users
            stakeholder_interviews: Structured feedback from key stakeholders
            usage_analytics: System usage patterns and metrics
            support_tickets: User-reported issues and requests
        &quot;&quot;&quot;
        stakeholder_evidence = {
            &#x27;user_satisfaction&#x27;: {
                &#x27;survey_responses&#x27;: len(user_surveys),
                &#x27;average_satisfaction&#x27;: self._calculate_average_satisfaction(user_surveys),
                &#x27;satisfaction_distribution&#x27;: self._analyze_satisfaction_distribution(user_surveys),
                &#x27;key_feedback_themes&#x27;: self._extract_feedback_themes(user_surveys)
            },
            &#x27;stakeholder_alignment&#x27;: {
                &#x27;interview_count&#x27;: len(stakeholder_interviews),
                &#x27;alignment_score&#x27;: self._assess_stakeholder_alignment(stakeholder_interviews),
                &#x27;priority_concerns&#x27;: self._identify_stakeholder_concerns(stakeholder_interviews),
                &#x27;feature_satisfaction&#x27;: self._analyze_feature_satisfaction(stakeholder_interviews)
            },
            &#x27;usage_patterns&#x27;: {
                &#x27;active_users&#x27;: usage_analytics.get(&#x27;active_users&#x27;, 0),
                &#x27;feature_adoption&#x27;: usage_analytics.get(&#x27;feature_usage&#x27;, {}),
                &#x27;user_retention&#x27;: usage_analytics.get(&#x27;retention_rate&#x27;, 0),
                &#x27;usage_trends&#x27;: self._analyze_usage_trends(usage_analytics)
            },
            &#x27;support_analysis&#x27;: {
                &#x27;ticket_volume&#x27;: len(support_tickets),
                &#x27;issue_categories&#x27;: self._categorize_support_issues(support_tickets),
                &#x27;resolution_satisfaction&#x27;: self._analyze_support_satisfaction(support_tickets),
                &#x27;recurring_issues&#x27;: self._identify_recurring_issues(support_tickets)
            }
        }

        self.evidence_sources[&#x27;stakeholder&#x27;] = stakeholder_evidence
        self.stakeholder_feedback = stakeholder_evidence

        # Map stakeholder evidence to evaluation criteria
        self._map_evidence_to_criteria(&#x27;stakeholder&#x27;, stakeholder_evidence)

    def _analyze_test_execution(self, test_results):
        &quot;&quot;&quot;Analyze test execution results for patterns and insights.&quot;&quot;&quot;
        if not test_results:
            return &quot;No test execution data available&quot;

        total = test_results.get(&#x27;total_tests&#x27;, 1)
        passed = test_results.get(&#x27;passed_tests&#x27;, 0)
        failed = test_results.get(&#x27;failed_tests&#x27;, 0)

        pass_rate = (passed / total) * 100 if total &gt; 0 else 0

        summary = f&quot;Test pass rate: {pass_rate:.1f}% ({passed}/{total} tests passed)&quot;

        if pass_rate &gt;= 95:
            summary += &quot; - Excellent test performance&quot;
        elif pass_rate &gt;= 85:
            summary += &quot; - Good test performance with minor issues&quot;
        elif pass_rate &gt;= 70:
            summary += &quot; - Moderate test performance requiring attention&quot;
        else:
            summary += &quot; - Poor test performance requiring significant work&quot;

        return summary

    def _analyze_performance_metrics(self, metrics):
        &quot;&quot;&quot;Analyze performance testing metrics for evaluation insights.&quot;&quot;&quot;
        if not metrics:
            return &quot;No performance data available&quot;

        response_times = metrics.get(&#x27;response_times&#x27;, {})
        avg_response = response_times.get(&#x27;average&#x27;, 0)

        if avg_response &lt; 1:
            performance_rating = &quot;Excellent&quot;
        elif avg_response &lt; 3:
            performance_rating = &quot;Good&quot;
        elif avg_response &lt; 5:
            performance_rating = &quot;Acceptable&quot;
        else:
            performance_rating = &quot;Poor&quot;

        return f&quot;Average response time: {avg_response:.2f}s - {performance_rating} performance&quot;

    def _analyze_defect_trends(self, defect_data):
        &quot;&quot;&quot;Analyze defect tracking data for quality insights.&quot;&quot;&quot;
        if not defect_data:
            return &quot;No defect data available&quot;

        defect_density = defect_data.get(&#x27;defects_per_kloc&#x27;, 0)
        critical_defects = defect_data.get(&#x27;critical_count&#x27;, 0)

        if defect_density &lt; 1 and critical_defects == 0:
            return &quot;Excellent code quality with minimal defects&quot;
        elif defect_density &lt; 3 and critical_defects &lt;= 1:
            return &quot;Good code quality with acceptable defect levels&quot;
        elif defect_density &lt; 5 and critical_defects &lt;= 3:
            return &quot;Moderate code quality requiring improvement&quot;
        else:
            return &quot;Poor code quality requiring significant attention&quot;

    def _calculate_average_satisfaction(self, surveys):
        &quot;&quot;&quot;Calculate average satisfaction score from survey responses.&quot;&quot;&quot;
        if not surveys:
            return 0

        total_score = sum(survey.get(&#x27;satisfaction_rating&#x27;, 3) for survey in surveys)
        return total_score / len(surveys)

    def _analyze_satisfaction_distribution(self, surveys):
        &quot;&quot;&quot;Analyze distribution of satisfaction ratings.&quot;&quot;&quot;
        if not surveys:
            return {}

        distribution = {&#x27;very_satisfied&#x27;: 0, &#x27;satisfied&#x27;: 0, &#x27;neutral&#x27;: 0, &#x27;dissatisfied&#x27;: 0, &#x27;very_dissatisfied&#x27;: 0}

        for survey in surveys:
            rating = survey.get(&#x27;satisfaction_rating&#x27;, 3)
            if rating &gt;= 4.5:
                distribution[&#x27;very_satisfied&#x27;] += 1
            elif rating &gt;= 3.5:
                distribution[&#x27;satisfied&#x27;] += 1
            elif rating &gt;= 2.5:
                distribution[&#x27;neutral&#x27;] += 1
            elif rating &gt;= 1.5:
                distribution[&#x27;dissatisfied&#x27;] += 1
            else:
                distribution[&#x27;very_dissatisfied&#x27;] += 1

        return distribution

    def _extract_feedback_themes(self, surveys):
        &quot;&quot;&quot;Extract common themes from survey feedback.&quot;&quot;&quot;
        if not surveys:
            return []

        # Simplified theme extraction - in practice would use NLP
        common_themes = []
        feedback_text = &#x27; &#x27;.join([survey.get(&#x27;comments&#x27;, &#x27;&#x27;) for survey in surveys]).lower()

        theme_keywords = {
            &#x27;usability&#x27;: [&#x27;easy&#x27;, &#x27;difficult&#x27;, &#x27;intuitive&#x27;, &#x27;confusing&#x27;, &#x27;user-friendly&#x27;],
            &#x27;performance&#x27;: [&#x27;fast&#x27;, &#x27;slow&#x27;, &#x27;responsive&#x27;, &#x27;loading&#x27;, &#x27;speed&#x27;],
            &#x27;reliability&#x27;: [&#x27;reliable&#x27;, &#x27;crashes&#x27;, &#x27;bugs&#x27;, &#x27;stable&#x27;, &#x27;errors&#x27;],
            &#x27;features&#x27;: [&#x27;features&#x27;, &#x27;functionality&#x27;, &#x27;missing&#x27;, &#x27;useful&#x27;, &#x27;unnecessary&#x27;]
        }

        for theme, keywords in theme_keywords.items():
            if any(keyword in feedback_text for keyword in keywords):
                common_themes.append(theme)

        return common_themes[:5]  # Return top 5 themes

    def _map_evidence_to_criteria(self, evidence_type, evidence_data):
        &quot;&quot;&quot;Map collected evidence to evaluation criteria.&quot;&quot;&quot;
        # This would map specific evidence to criteria based on relationships
        # Simplified implementation for demonstration
        pass

    def compare_expected_vs_actual_outputs(self, test_scenarios, expected_outputs, actual_outputs):
        &quot;&quot;&quot;
        Compare expected vs actual outputs to identify discrepancies and issues.

        Args:
            test_scenarios: Test scenarios that were executed
            expected_outputs: Expected results for each scenario
            actual_outputs: Actual results produced by the system
        &quot;&quot;&quot;
        output_analysis = {
            &#x27;total_scenarios&#x27;: len(test_scenarios),
            &#x27;exact_matches&#x27;: 0,
            &#x27;partial_matches&#x27;: 0,
            &#x27;discrepancies&#x27;: [],
            &#x27;error_patterns&#x27;: [],
            &#x27;recommendations&#x27;: []
        }

        for i, scenario in enumerate(test_scenarios):
            scenario_id = scenario.get(&#x27;id&#x27;, f&quot;scenario_{i+1}&quot;)
            expected = expected_outputs.get(scenario_id, &#x27;&#x27;)
            actual = actual_outputs.get(scenario_id, &#x27;&#x27;)

            comparison_result = self._compare_outputs(scenario_id, expected, actual, scenario)

            if comparison_result[&#x27;match_type&#x27;] == &#x27;exact&#x27;:
                output_analysis[&#x27;exact_matches&#x27;] += 1
            elif comparison_result[&#x27;match_type&#x27;] == &#x27;partial&#x27;:
                output_analysis[&#x27;partial_matches&#x27;] += 1
            else:
                output_analysis[&#x27;discrepancies&#x27;].append(comparison_result)

        # Analyze patterns in discrepancies
        output_analysis[&#x27;error_patterns&#x27;] = self._identify_error_patterns(
            output_analysis[&#x27;discrepancies&#x27;]
        )

        # Generate recommendations based on analysis
        output_analysis[&#x27;recommendations&#x27;] = self._generate_output_recommendations(
            output_analysis
        )

        self.test_results[&#x27;output_analysis&#x27;] = output_analysis
        return output_analysis

    def _compare_outputs(self, scenario_id, expected, actual, scenario_context):
        &quot;&quot;&quot;Compare expected and actual outputs for a single scenario.&quot;&quot;&quot;
        comparison = {
            &#x27;scenario_id&#x27;: scenario_id,
            &#x27;scenario_description&#x27;: scenario_context.get(&#x27;description&#x27;, &#x27;&#x27;),
            &#x27;expected&#x27;: expected,
            &#x27;actual&#x27;: actual,
            &#x27;match_type&#x27;: &#x27;none&#x27;,
            &#x27;discrepancy_details&#x27;: [],
            &#x27;severity&#x27;: &#x27;low&#x27;,
            &#x27;root_cause_hypothesis&#x27;: &#x27;&#x27;
        }

        # Exact match check
        if str(expected).strip() == str(actual).strip():
            comparison[&#x27;match_type&#x27;] = &#x27;exact&#x27;
            return comparison

        # Partial match analysis
        expected_str = str(expected).lower().strip()
        actual_str = str(actual).lower().strip()

        # Check for partial content match
        if expected_str in actual_str or actual_str in expected_str:
            comparison[&#x27;match_type&#x27;] = &#x27;partial&#x27;
            comparison[&#x27;discrepancy_details&#x27;].append(&#x27;Partial content match detected&#x27;)

        # Identify specific discrepancy types
        if expected_str and not actual_str:
            comparison[&#x27;discrepancy_details&#x27;].append(&#x27;No output produced when output expected&#x27;)
            comparison[&#x27;severity&#x27;] = &#x27;high&#x27;
        elif not expected_str and actual_str:
            comparison[&#x27;discrepancy_details&#x27;].append(&#x27;Unexpected output produced&#x27;)
            comparison[&#x27;severity&#x27;] = &#x27;medium&#x27;
        elif expected_str != actual_str:
            comparison[&#x27;discrepancy_details&#x27;].append(&#x27;Output content differs from expected&#x27;)
            comparison[&#x27;severity&#x27;] = &#x27;medium&#x27;

        # Generate root cause hypothesis
        comparison[&#x27;root_cause_hypothesis&#x27;] = self._hypothesize_root_cause(
            expected, actual, scenario_context
        )

        return comparison

    def _hypothesize_root_cause(self, expected, actual, scenario_context):
        &quot;&quot;&quot;Generate hypothesis about root cause of output discrepancy.&quot;&quot;&quot;
        expected_str = str(expected).lower()
        actual_str = str(actual).lower()

        # Data processing issues
        if &#x27;error&#x27; in actual_str or &#x27;exception&#x27; in actual_str:
            return &quot;System error or exception during processing&quot;

        # Formatting issues
        if expected_str.replace(&#x27; &#x27;, &#x27;&#x27;) == actual_str.replace(&#x27; &#x27;, &#x27;&#x27;):
            return &quot;Output formatting or whitespace differences&quot;

        # Calculation issues
        if any(char.isdigit() for char in expected_str) and any(char.isdigit() for char in actual_str):
            return &quot;Possible calculation or numeric processing error&quot;

        # Logic issues
        if scenario_context.get(&#x27;type&#x27;) == &#x27;business_logic&#x27;:
            return &quot;Business logic implementation may not match requirements&quot;

        return &quot;Root cause requires further investigation&quot;

    def _identify_error_patterns(self, discrepancies):
        &quot;&quot;&quot;Identify common patterns across output discrepancies.&quot;&quot;&quot;
        if not discrepancies:
            return []

        patterns = []

        # Group by severity
        high_severity_count = sum(1 for d in discrepancies if d[&#x27;severity&#x27;] == &#x27;high&#x27;)
        if high_severity_count &gt; len(discrepancies) * 0.3:
            patterns.append(&quot;High proportion of severe output discrepancies&quot;)

        # Group by root cause
        root_causes = [d[&#x27;root_cause_hypothesis&#x27;] for d in discrepancies]
        common_causes = {}
        for cause in root_causes:
            common_causes[cause] = common_causes.get(cause, 0) + 1

        for cause, count in common_causes.items():
            if count &gt;= 2:
                patterns.append(f&quot;Multiple instances of: {cause}&quot;)

        return patterns

    def _generate_output_recommendations(self, analysis):
        &quot;&quot;&quot;Generate recommendations based on output analysis.&quot;&quot;&quot;
        recommendations = []

        total_scenarios = analysis[&#x27;total_scenarios&#x27;]
        exact_matches = analysis[&#x27;exact_matches&#x27;]

        if total_scenarios &gt; 0:
            success_rate = (exact_matches / total_scenarios) * 100

            if success_rate &lt; 70:
                recommendations.append(&quot;Critical: Low output accuracy requires immediate attention&quot;)
                recommendations.append(&quot;Review and strengthen testing procedures&quot;)
                recommendations.append(&quot;Conduct thorough requirement analysis and implementation review&quot;)
            elif success_rate &lt; 90:
                recommendations.append(&quot;Moderate: Some output discrepancies need resolution&quot;)
                recommendations.append(&quot;Focus on specific failing scenarios and root causes&quot;)
            else:
                recommendations.append(&quot;Good: Minor output discrepancies can be addressed incrementally&quot;)

        # Pattern-specific recommendations
        for pattern in analysis[&#x27;error_patterns&#x27;]:
            if &#x27;calculation&#x27; in pattern.lower():
                recommendations.append(&quot;Review mathematical calculations and numeric processing logic&quot;)
            elif &#x27;formatting&#x27; in pattern.lower():
                recommendations.append(&quot;Standardize output formatting and data presentation&quot;)
            elif &#x27;business logic&#x27; in pattern.lower():
                recommendations.append(&quot;Validate business logic implementation against requirements&quot;)

        return recommendations

    def generate_evaluation_report(self, include_executive_summary=True, 
                                 include_detailed_findings=True, include_recommendations=True):
        &quot;&quot;&quot;
        Generate comprehensive evaluation report synthesizing all evidence and analysis.

        Args:
            include_executive_summary: Include high-level summary for executives
            include_detailed_findings: Include detailed analysis and evidence
            include_recommendations: Include actionable recommendations
        &quot;&quot;&quot;
        report = f&quot;&quot;&quot;
SOFTWARE SOLUTION EVALUATION REPORT
Solution: {self.solution_name}
Evaluation Scope: {self.evaluation_scope}
Report Date: 2025-09-20
&quot;&quot;&quot;

        if include_executive_summary:
            report += self._generate_executive_summary()

        if include_detailed_findings:
            report += self._generate_detailed_findings()

        if include_recommendations:
            report += self._generate_recommendations_section()

        # Add reflection section
        report += self._generate_reflection_section()

        return report

    def _generate_executive_summary(self):
        &quot;&quot;&quot;Generate executive summary of evaluation findings.&quot;&quot;&quot;
        summary = &quot;\n\nEXECUTIVE SUMMARY\n&quot; + &quot;=&quot;*50 + &quot;\n&quot;

        # Overall assessment
        overall_score = self._calculate_overall_evaluation_score()
        summary += f&quot;\nOverall Evaluation Score: {overall_score:.1%}\n&quot;

        if overall_score &gt;= 0.85:
            summary += &quot;Assessment: Excellent - Solution exceeds expectations\n&quot;
        elif overall_score &gt;= 0.75:
            summary += &quot;Assessment: Good - Solution meets most requirements with minor improvements needed\n&quot;
        elif overall_score &gt;= 0.60:
            summary += &quot;Assessment: Satisfactory - Solution meets basic requirements but needs significant improvements\n&quot;
        else:
            summary += &quot;Assessment: Unsatisfactory - Solution requires major revisions before deployment\n&quot;

        # Key highlights
        summary += &quot;\nKey Findings:\n&quot;

        # Testing results
        if &#x27;testing&#x27; in self.evidence_sources:
            testing_data = self.evidence_sources[&#x27;testing&#x27;]
            test_summary = testing_data.get(&#x27;test_execution&#x27;, {}).get(&#x27;execution_summary&#x27;, &#x27;&#x27;)
            summary += f&quot;‚Ä¢ Testing: {test_summary}\n&quot;

        # Stakeholder satisfaction
        if &#x27;stakeholder&#x27; in self.evidence_sources:
            stakeholder_data = self.evidence_sources[&#x27;stakeholder&#x27;]
            avg_satisfaction = stakeholder_data.get(&#x27;user_satisfaction&#x27;, {}).get(&#x27;average_satisfaction&#x27;, 0)
            summary += f&quot;‚Ä¢ User Satisfaction: {avg_satisfaction:.1f}/5.0\n&quot;

        # Critical issues
        if self.test_results.get(&#x27;output_analysis&#x27;):
            output_analysis = self.test_results[&#x27;output_analysis&#x27;]
            total_scenarios = output_analysis.get(&#x27;total_scenarios&#x27;, 0)
            exact_matches = output_analysis.get(&#x27;exact_matches&#x27;, 0)
            if total_scenarios &gt; 0:
                accuracy = (exact_matches / total_scenarios) * 100
                summary += f&quot;‚Ä¢ Output Accuracy: {accuracy:.1f}%\n&quot;

        return summary

    def _generate_detailed_findings(self):
        &quot;&quot;&quot;Generate detailed findings section of evaluation report.&quot;&quot;&quot;
        findings = &quot;\n\nDETAILED FINDINGS\n&quot; + &quot;=&quot;*50 + &quot;\n&quot;

        # Criteria evaluation results
        findings += &quot;\nEvaluation Criteria Results:\n&quot;
        findings += &quot;-&quot; * 30 + &quot;\n&quot;

        for category, criteria_list in self.evaluation_criteria.items():
            findings += f&quot;\n{category.replace(&#x27;_&#x27;, &#x27; &#x27;).title()} Criteria:\n&quot;

            for criterion in criteria_list:
                status = criterion.get(&#x27;evaluation_status&#x27;, &#x27;pending&#x27;)
                target = criterion.get(&#x27;target_value&#x27;, &#x27;TBD&#x27;)
                actual = criterion.get(&#x27;actual_value&#x27;, &#x27;Not measured&#x27;)

                findings += f&quot;‚Ä¢ {criterion[&#x27;description&#x27;]}\n&quot;
                findings += f&quot;  Target: {target} | Actual: {actual} | Status: {status}\n&quot;

        # Evidence summary
        if self.evidence_sources:
            findings += &quot;\nEvidence Summary:\n&quot;
            findings += &quot;-&quot; * 30 + &quot;\n&quot;

            for source_type, evidence in self.evidence_sources.items():
                findings += f&quot;\n{source_type.title()} Evidence:\n&quot;
                findings += self._summarize_evidence(evidence)

        # Output analysis
        if self.test_results.get(&#x27;output_analysis&#x27;):
            findings += &quot;\nOutput Analysis:\n&quot;
            findings += &quot;-&quot; * 30 + &quot;\n&quot;
            output_analysis = self.test_results[&#x27;output_analysis&#x27;]

            findings += f&quot;Total Test Scenarios: {output_analysis.get(&#x27;total_scenarios&#x27;, 0)}\n&quot;
            findings += f&quot;Exact Matches: {output_analysis.get(&#x27;exact_matches&#x27;, 0)}\n&quot;
            findings += f&quot;Partial Matches: {output_analysis.get(&#x27;partial_matches&#x27;, 0)}\n&quot;
            findings += f&quot;Discrepancies: {len(output_analysis.get(&#x27;discrepancies&#x27;, []))}\n&quot;

            if output_analysis.get(&#x27;error_patterns&#x27;):
                findings += &quot;\nIdentified Error Patterns:\n&quot;
                for pattern in output_analysis[&#x27;error_patterns&#x27;]:
                    findings += f&quot;‚Ä¢ {pattern}\n&quot;

        return findings

    def _generate_recommendations_section(self):
        &quot;&quot;&quot;Generate recommendations section of evaluation report.&quot;&quot;&quot;
        recommendations = &quot;\n\nRECOMMENDATIONS\n&quot; + &quot;=&quot;*50 + &quot;\n&quot;

        # Collect recommendations from various sources
        all_recommendations = []

        # Output analysis recommendations
        if self.test_results.get(&#x27;output_analysis&#x27;, {}).get(&#x27;recommendations&#x27;):
            all_recommendations.extend(self.test_results[&#x27;output_analysis&#x27;][&#x27;recommendations&#x27;])

        # Add general recommendations based on evaluation
        if self.recommendations:
            all_recommendations.extend(self.recommendations)

        # Prioritize recommendations
        prioritized_recommendations = self._prioritize_recommendations(all_recommendations)

        recommendations += &quot;\nHigh Priority:\n&quot;
        for rec in prioritized_recommendations.get(&#x27;high&#x27;, []):
            recommendations += f&quot;‚Ä¢ {rec}\n&quot;

        recommendations += &quot;\nMedium Priority:\n&quot;
        for rec in prioritized_recommendations.get(&#x27;medium&#x27;, []):
            recommendations += f&quot;‚Ä¢ {rec}\n&quot;

        recommendations += &quot;\nLow Priority:\n&quot;
        for rec in prioritized_recommendations.get(&#x27;low&#x27;, []):
            recommendations += f&quot;‚Ä¢ {rec}\n&quot;

        return recommendations

    def _generate_reflection_section(self):
        &quot;&quot;&quot;Generate reflection section covering maintainability, deployment, and lessons learned.&quot;&quot;&quot;
        reflection = &quot;\n\nREFLECTION AND LESSONS LEARNED\n&quot; + &quot;=&quot;*50 + &quot;\n&quot;

        reflection += &quot;\nMaintainability Assessment:\n&quot;
        reflection += self._assess_maintainability()

        reflection += &quot;\nDeployment Readiness:\n&quot;
        reflection += self._assess_deployment_readiness()

        reflection += &quot;\nLessons Learned:\n&quot;
        reflection += self._document_lessons_learned()

        reflection += &quot;\nFuture Considerations:\n&quot;
        reflection += self._identify_future_considerations()

        return reflection

    def _calculate_overall_evaluation_score(self):
        &quot;&quot;&quot;Calculate weighted overall evaluation score.&quot;&quot;&quot;
        if not self.evaluation_criteria:
            return 0.5  # Default neutral score

        total_weight = 0
        weighted_score = 0

        for category, criteria_list in self.evaluation_criteria.items():
            for criterion in criteria_list:
                weight = criterion.get(&#x27;weight&#x27;, 0.2)

                # Mock scoring based on status - in practice would use actual measurements
                status = criterion.get(&#x27;evaluation_status&#x27;, &#x27;pending&#x27;)
                if status == &#x27;passed&#x27; or status == &#x27;excellent&#x27;:
                    score = 1.0
                elif status == &#x27;good&#x27; or status == &#x27;acceptable&#x27;:
                    score = 0.8
                elif status == &#x27;fair&#x27; or status == &#x27;needs_improvement&#x27;:
                    score = 0.6
                elif status == &#x27;poor&#x27; or status == &#x27;failed&#x27;:
                    score = 0.3
                else:
                    score = 0.5  # Pending/unknown

                weighted_score += score * weight
                total_weight += weight

        return weighted_score / total_weight if total_weight &gt; 0 else 0.5

    def _summarize_evidence(self, evidence):
        &quot;&quot;&quot;Generate summary of evidence for reporting.&quot;&quot;&quot;
        summary = &quot;&quot;

        if isinstance(evidence, dict):
            for key, value in evidence.items():
                if isinstance(value, dict):
                    summary += f&quot;  {key.replace(&#x27;_&#x27;, &#x27; &#x27;).title()}: Multiple metrics collected\n&quot;
                elif isinstance(value, (int, float)):
                    summary += f&quot;  {key.replace(&#x27;_&#x27;, &#x27; &#x27;).title()}: {value}\n&quot;
                elif isinstance(value, str):
                    summary += f&quot;  {key.replace(&#x27;_&#x27;, &#x27; &#x27;).title()}: {value}\n&quot;

        return summary

    def _prioritize_recommendations(self, recommendations):
        &quot;&quot;&quot;Prioritize recommendations by urgency and impact.&quot;&quot;&quot;
        prioritized = {&#x27;high&#x27;: [], &#x27;medium&#x27;: [], &#x27;low&#x27;: []}

        high_priority_keywords = [&#x27;critical&#x27;, &#x27;immediate&#x27;, &#x27;security&#x27;, &#x27;data loss&#x27;, &#x27;major&#x27;]
        medium_priority_keywords = [&#x27;moderate&#x27;, &#x27;improve&#x27;, &#x27;enhance&#x27;, &#x27;optimize&#x27;]

        for rec in recommendations:
            rec_lower = rec.lower()

            if any(keyword in rec_lower for keyword in high_priority_keywords):
                prioritized[&#x27;high&#x27;].append(rec)
            elif any(keyword in rec_lower for keyword in medium_priority_keywords):
                prioritized[&#x27;medium&#x27;].append(rec)
            else:
                prioritized[&#x27;low&#x27;].append(rec)

        return prioritized

    def _assess_maintainability(self):
        &quot;&quot;&quot;Assess solution maintainability for reflection.&quot;&quot;&quot;
        assessment = &quot;&quot;

        # Code quality factors
        assessment += &quot;‚Ä¢ Code Structure: &quot;
        if &#x27;technical&#x27; in self.evidence_sources:
            assessment += &quot;Well-organized with clear separation of concerns\n&quot;
        else:
            assessment += &quot;Assessment pending - code review needed\n&quot;

        assessment += &quot;‚Ä¢ Documentation: &quot;
        assessment += &quot;Comprehensive documentation supports maintenance activities\n&quot;

        assessment += &quot;‚Ä¢ Testing Coverage: &quot;
        if &#x27;testing&#x27; in self.evidence_sources:
            testing_data = self.evidence_sources[&#x27;testing&#x27;]
            coverage = testing_data.get(&#x27;test_execution&#x27;, {}).get(&#x27;test_coverage&#x27;, 0)
            assessment += f&quot;{coverage}% test coverage provides good maintainability support\n&quot;
        else:
            assessment += &quot;Test coverage assessment needed\n&quot;

        return assessment

    def _assess_deployment_readiness(self):
        &quot;&quot;&quot;Assess deployment readiness for reflection.&quot;&quot;&quot;
        readiness = &quot;&quot;

        # Technical readiness
        readiness += &quot;‚Ä¢ Technical Infrastructure: &quot;
        readiness += &quot;Production environment configured and tested\n&quot;

        # User readiness
        readiness += &quot;‚Ä¢ User Preparation: &quot;
        if &#x27;stakeholder&#x27; in self.evidence_sources:
            readiness += &quot;User training completed and feedback incorporated\n&quot;
        else:
            readiness += &quot;User preparation assessment needed\n&quot;

        # Process readiness
        readiness += &quot;‚Ä¢ Operational Procedures: &quot;
        readiness += &quot;Deployment and rollback procedures documented and tested\n&quot;

        return readiness

    def _document_lessons_learned(self):
        &quot;&quot;&quot;Document key lessons learned during development and evaluation.&quot;&quot;&quot;
        lessons = &quot;&quot;

        lessons += &quot;‚Ä¢ Requirements Management: &quot;
        lessons += &quot;Early stakeholder engagement critical for requirement clarity\n&quot;

        lessons += &quot;‚Ä¢ Testing Strategy: &quot;
        lessons += &quot;Comprehensive testing prevents deployment issues and reduces rework\n&quot;

        lessons += &quot;‚Ä¢ User Feedback: &quot;
        lessons += &quot;Regular user feedback throughout development improves final solution quality\n&quot;

        lessons += &quot;‚Ä¢ Technical Decisions: &quot;
        lessons += &quot;Architecture decisions impact long-term maintainability and scalability\n&quot;

        return lessons

    def _identify_future_considerations(self):
        &quot;&quot;&quot;Identify considerations for future development and improvements.&quot;&quot;&quot;
        considerations = &quot;&quot;

        considerations += &quot;‚Ä¢ Scalability: &quot;
        considerations += &quot;Monitor performance as user base grows and plan capacity expansion\n&quot;

        considerations += &quot;‚Ä¢ Feature Evolution: &quot;
        considerations += &quot;Establish process for evaluating and implementing new feature requests\n&quot;

        considerations += &quot;‚Ä¢ Technology Updates: &quot;
        considerations += &quot;Plan for regular updates to underlying technologies and dependencies\n&quot;

        considerations += &quot;‚Ä¢ User Support: &quot;
        considerations += &quot;Maintain robust user support processes and knowledge base\n&quot;

        return considerations

def demonstrate_solution_evaluation():
    &quot;&quot;&quot;
    Practical example of evaluating a school management system solution.
    &quot;&quot;&quot;
    print(&quot;SOLUTION EVALUATION EXAMPLE&quot;)
    print(&quot;=&quot; * 29)

    # Create solution evaluator
    evaluator = SolutionEvaluator(
        &quot;School Management System&quot;,
        &quot;Complete system evaluation including functionality, performance, and user satisfaction&quot;
    )

    # Define evaluation criteria
    evaluator.define_evaluation_criteria(
        functional_criteria=[
            &quot;Student enrollment and registration processes work correctly&quot;,
            &quot;Grade recording and calculation accuracy is 100%&quot;,
            &quot;Parent portal displays real-time student information&quot;,
            &quot;Teacher interface supports all required grading workflows&quot;,
            &quot;Administrative reporting generates required compliance reports&quot;
        ],
        non_functional_criteria=[
            &quot;System response time under 3 seconds for 95% of operations&quot;,
            &quot;System supports 500 concurrent users without performance degradation&quot;,
            &quot;System availability of 99.5% during school hours&quot;,
            &quot;Data backup and recovery procedures work within 4 hours&quot;,
            &quot;User interface meets WCAG 2.1 AA accessibility standards&quot;
        ],
        stakeholder_criteria=[
            &quot;Teacher satisfaction score above 4.0/5.0&quot;,
            &quot;Parent satisfaction with information access above 4.2/5.0&quot;,
            &quot;Administrative staff report 30% reduction in manual tasks&quot;,
            &quot;Student users find interface intuitive and helpful&quot;,
            &quot;IT support tickets reduced by 50% compared to previous system&quot;
        ],
        technical_criteria=[
            &quot;Code coverage above 80% with comprehensive test suite&quot;,
            &quot;Security vulnerability scan shows zero critical issues&quot;,
            &quot;Database performance optimized with query response under 100ms&quot;,
            &quot;System architecture supports horizontal scaling&quot;,
            &quot;Integration with existing school systems functions correctly&quot;
        ],
        business_criteria=[
            &quot;Implementation completed within budget and timeline&quot;,
            &quot;ROI achieved within 18 months of deployment&quot;,
            &quot;Compliance with education sector regulations maintained&quot;,
            &quot;Staff training costs remain under 10% of total project cost&quot;,
            &quot;System reduces overall administrative costs by 25%&quot;
        ]
    )

    # Collect testing evidence
    evaluator.collect_evidence_from_testing(
        test_plan_results={
            &#x27;total_tests&#x27;: 245,
            &#x27;passed_tests&#x27;: 231,
            &#x27;failed_tests&#x27;: 14,
            &#x27;test_categories&#x27;: {
                &#x27;functional&#x27;: {&#x27;passed&#x27;: 98, &#x27;failed&#x27;: 2},
                &#x27;integration&#x27;: {&#x27;passed&#x27;: 67, &#x27;failed&#x27;: 8},
                &#x27;performance&#x27;: {&#x27;passed&#x27;: 44, &#x27;failed&#x27;: 1},
                &#x27;security&#x27;: {&#x27;passed&#x27;: 22, &#x27;failed&#x27;: 3}
            }
        },
        test_coverage_data={
            &#x27;percentage&#x27;: 82,
            &#x27;lines_covered&#x27;: 12543,
            &#x27;total_lines&#x27;: 15298,
            &#x27;uncovered_areas&#x27;: [&#x27;Error handling&#x27;, &#x27;Legacy data migration&#x27;]
        },
        performance_metrics={
            &#x27;response_times&#x27;: {
                &#x27;average&#x27;: 1.8,
                &#x27;median&#x27;: 1.2,
                &#x27;p95&#x27;: 2.9,
                &#x27;p99&#x27;: 4.1
            },
            &#x27;throughput&#x27;: {
                &#x27;requests_per_second&#x27;: 150,
                &#x27;concurrent_users_tested&#x27;: 300
            },
            &#x27;resource_usage&#x27;: {
                &#x27;cpu_utilization&#x27;: 65,
                &#x27;memory_usage&#x27;: 78,
                &#x27;database_connections&#x27;: 85
            }
        },
        defect_tracking_data={
            &#x27;defects_per_kloc&#x27;: 2.3,
            &#x27;avg_resolution_time&#x27;: 3.2,
            &#x27;critical_count&#x27;: 2,
            &#x27;total_defects&#x27;: 28,
            &#x27;resolved_defects&#x27;: 24
        }
    )

    # Collect stakeholder feedback
    evaluator.collect_stakeholder_feedback(
        user_surveys=[
            {&#x27;user_type&#x27;: &#x27;teacher&#x27;, &#x27;satisfaction_rating&#x27;: 4.2, &#x27;comments&#x27;: &#x27;Much easier than old system&#x27;},
            {&#x27;user_type&#x27;: &#x27;teacher&#x27;, &#x27;satisfaction_rating&#x27;: 3.8, &#x27;comments&#x27;: &#x27;Some features still confusing&#x27;},
            {&#x27;user_type&#x27;: &#x27;parent&#x27;, &#x27;satisfaction_rating&#x27;: 4.5, &#x27;comments&#x27;: &#x27;Love real-time grade access&#x27;},
            {&#x27;user_type&#x27;: &#x27;parent&#x27;, &#x27;satisfaction_rating&#x27;: 4.1, &#x27;comments&#x27;: &#x27;Mobile app works great&#x27;},
            {&#x27;user_type&#x27;: &#x27;admin&#x27;, &#x27;satisfaction_rating&#x27;: 4.3, &#x27;comments&#x27;: &#x27;Reports are much faster now&#x27;}
        ],
        stakeholder_interviews=[
            {&#x27;stakeholder&#x27;: &#x27;Principal&#x27;, &#x27;feedback&#x27;: &#x27;Significant improvement in efficiency&#x27;},
            {&#x27;stakeholder&#x27;: &#x27;IT Manager&#x27;, &#x27;feedback&#x27;: &#x27;Integration went smoothly overall&#x27;},
            {&#x27;stakeholder&#x27;: &#x27;Department Head&#x27;, &#x27;feedback&#x27;: &#x27;Teachers adapting well to new workflows&#x27;}
        ],
        usage_analytics={
            &#x27;active_users&#x27;: 1247,
            &#x27;daily_logins&#x27;: 892,
            &#x27;feature_usage&#x27;: {
                &#x27;gradebook&#x27;: 95,
                &#x27;attendance&#x27;: 87,
                &#x27;messaging&#x27;: 72,
                &#x27;reports&#x27;: 68
            },
            &#x27;retention_rate&#x27;: 94
        },
        support_tickets=[
            {&#x27;category&#x27;: &#x27;login_issues&#x27;, &#x27;count&#x27;: 23, &#x27;avg_resolution&#x27;: 2.1},
            {&#x27;category&#x27;: &#x27;grade_entry&#x27;, &#x27;count&#x27;: 15, &#x27;avg_resolution&#x27;: 1.8},
            {&#x27;category&#x27;: &#x27;report_generation&#x27;, &#x27;count&#x27;: 12, &#x27;avg_resolution&#x27;: 3.2},
            {&#x27;category&#x27;: &#x27;mobile_app&#x27;, &#x27;count&#x27;: 8, &#x27;avg_resolution&#x27;: 2.5}
        ]
    )

    # Compare expected vs actual outputs for key scenarios
    test_scenarios = [
        {&#x27;id&#x27;: &#x27;grade_calculation&#x27;, &#x27;description&#x27;: &#x27;Calculate semester GPA for student&#x27;},
        {&#x27;id&#x27;: &#x27;attendance_report&#x27;, &#x27;description&#x27;: &#x27;Generate monthly attendance report&#x27;},
        {&#x27;id&#x27;: &#x27;parent_notification&#x27;, &#x27;description&#x27;: &#x27;Send grade update notification to parent&#x27;},
        {&#x27;id&#x27;: &#x27;schedule_conflict&#x27;, &#x27;description&#x27;: &#x27;Detect and report scheduling conflicts&#x27;}
    ]

    expected_outputs = {
        &#x27;grade_calculation&#x27;: &#x27;3.75 GPA (A- average)&#x27;,
        &#x27;attendance_report&#x27;: &#x27;Student A: 95% attendance (19/20 days)&#x27;,
        &#x27;parent_notification&#x27;: &#x27;Email sent: Math quiz grade posted (B+)&#x27;,
        &#x27;schedule_conflict&#x27;: &#x27;Conflict detected: Room 101 double-booked at 2:00 PM&#x27;
    }

    actual_outputs = {
        &#x27;grade_calculation&#x27;: &#x27;3.75 GPA (A- average)&#x27;,
        &#x27;attendance_report&#x27;: &#x27;Student A: 95% attendance (19 of 20 days)&#x27;,
        &#x27;parent_notification&#x27;: &#x27;Email sent: Math quiz grade posted (B+)&#x27;,
        &#x27;schedule_conflict&#x27;: &#x27;No conflicts detected&#x27;
    }

    output_analysis = evaluator.compare_expected_vs_actual_outputs(
        test_scenarios, expected_outputs, actual_outputs
    )

    # Generate comprehensive evaluation report
    evaluation_report = evaluator.generate_evaluation_report()
    print(evaluation_report)

    print(f&quot;\nOUTPUT ANALYSIS SUMMARY:&quot;)
    print(f&quot;Total Scenarios: {output_analysis[&#x27;total_scenarios&#x27;]}&quot;)
    print(f&quot;Exact Matches: {output_analysis[&#x27;exact_matches&#x27;]}&quot;)
    print(f&quot;Discrepancies: {len(output_analysis[&#x27;discrepancies&#x27;])}&quot;)

    if output_analysis[&#x27;discrepancies&#x27;]:
        print(&quot;\nKey Discrepancies:&quot;)
        for discrepancy in output_analysis[&#x27;discrepancies&#x27;][:3]:
            print(f&quot;‚Ä¢ {discrepancy[&#x27;scenario_description&#x27;]}: {discrepancy[&#x27;root_cause_hypothesis&#x27;]}&quot;)

    return evaluator

# Run demonstration
if __name__ == &quot;__main__&quot;:
    evaluation_demo = demonstrate_solution_evaluation()
</code></pre>
</div>
<hr />
<h2 id="test-plan-development-and-testing-outcomes">Test plan development and testing outcomes<a class="headerlink" href="#test-plan-development-and-testing-outcomes" title="Permanent link">&para;</a></h2>
<p>Effective evaluation requires systematic testing that produces reliable evidence about solution performance. Test plans serve as blueprints for gathering objective data that informs evaluation decisions and identifies areas for improvement.</p>
<h3 id="test-plan-integration-with-evaluation">Test plan integration with evaluation<a class="headerlink" href="#test-plan-integration-with-evaluation" title="Permanent link">&para;</a></h3>
<p><strong>Test plans for evaluation</strong> differ from development testing by focusing on evidence collection for decision-making rather than defect detection. Evaluation testing assesses whether the solution meets defined criteria and stakeholder expectations.</p>
<p><strong>Key components of evaluation-focused test plans:</strong></p>
<ul>
<li>
<p><strong>Requirement traceability</strong>: Link test cases directly to evaluation criteria</p>
</li>
<li>
<p><strong>Evidence collection</strong>: Design tests to produce measurable evidence</p>
</li>
<li>
<p><strong>Stakeholder scenarios</strong>: Include real-world usage patterns from different user types</p>
</li>
<li>
<p><strong>Boundary and edge cases</strong>: Test limits and exceptional conditions</p>
</li>
<li>
<p><strong>Performance baselines</strong>: Establish metrics for non-functional requirements</p>
</li>
</ul>
<div class="highlight python-template" data-fence-type="template" data-language="python">
<pre><code class="language-python">class EvaluationTestPlanner:
    &quot;&quot;&quot;
    Framework for developing test plans specifically focused on solution evaluation.
    Links testing activities directly to evaluation criteria and evidence collection.
    &quot;&quot;&quot;

    def __init__(self, solution_name, evaluation_objectives):
        self.solution_name = solution_name
        self.evaluation_objectives = evaluation_objectives
        self.test_plan = {}
        self.test_cases = {}
        self.evaluation_evidence = {}
        self.testing_outcomes = {}

    def create_evaluation_test_plan(self, functional_requirements, non_functional_requirements,
                                  stakeholder_scenarios, evaluation_criteria):
        &quot;&quot;&quot;
        Create comprehensive test plan aligned with evaluation objectives.

        Args:
            functional_requirements: Functional behaviors to validate
            non_functional_requirements: Performance, usability, security requirements
            stakeholder_scenarios: Real-world usage scenarios from different user types
            evaluation_criteria: Specific criteria that testing will provide evidence for
        &quot;&quot;&quot;
        self.test_plan = {
            &#x27;plan_overview&#x27;: {
                &#x27;solution&#x27;: self.solution_name,
                &#x27;objectives&#x27;: self.evaluation_objectives,
                &#x27;evaluation_focus&#x27;: &#x27;Evidence collection for solution effectiveness assessment&#x27;,
                &#x27;testing_approach&#x27;: &#x27;Mixed: automated functional testing, manual scenario testing, performance testing&#x27;
            },
            &#x27;functional_test_suite&#x27;: self._plan_functional_testing(functional_requirements, evaluation_criteria),
            &#x27;non_functional_test_suite&#x27;: self._plan_non_functional_testing(non_functional_requirements, evaluation_criteria),
            &#x27;scenario_test_suite&#x27;: self._plan_scenario_testing(stakeholder_scenarios, evaluation_criteria),
            &#x27;boundary_test_suite&#x27;: self._plan_boundary_testing(functional_requirements),
            &#x27;integration_test_suite&#x27;: self._plan_integration_testing(),
            &#x27;evidence_collection_plan&#x27;: self._plan_evidence_collection(evaluation_criteria)
        }

        return self.test_plan

    def _plan_functional_testing(self, requirements, criteria):
        &quot;&quot;&quot;Plan functional testing to validate requirement compliance.&quot;&quot;&quot;
        functional_suite = {
            &#x27;suite_purpose&#x27;: &#x27;Validate functional requirement compliance for evaluation&#x27;,
            &#x27;test_categories&#x27;: [],
            &#x27;requirement_coverage&#x27;: {},
            &#x27;evidence_targets&#x27;: []
        }

        for req in requirements:
            req_id = req.get(&#x27;id&#x27;, f&quot;req_{len(functional_suite[&#x27;test_categories&#x27;]) + 1}&quot;)

            test_category = {
                &#x27;requirement_id&#x27;: req_id,
                &#x27;requirement_description&#x27;: req[&#x27;description&#x27;],
                &#x27;test_cases&#x27;: self._generate_functional_test_cases(req),
                &#x27;evaluation_criteria_mapping&#x27;: self._map_requirement_to_criteria(req, criteria),
                &#x27;evidence_collection&#x27;: {
                    &#x27;success_metrics&#x27;: [&#x27;Pass/fail rate&#x27;, &#x27;Requirement coverage&#x27;, &#x27;Defect density&#x27;],
                    &#x27;data_points&#x27;: [&#x27;Test execution results&#x27;, &#x27;Error logs&#x27;, &#x27;Performance during execution&#x27;],
                    &#x27;evaluation_impact&#x27;: &#x27;Validates functional criterion compliance&#x27;
                }
            }

            functional_suite[&#x27;test_categories&#x27;].append(test_category)
            functional_suite[&#x27;requirement_coverage&#x27;][req_id] = len(test_category[&#x27;test_cases&#x27;])

        return functional_suite

    def _plan_non_functional_testing(self, nf_requirements, criteria):
        &quot;&quot;&quot;Plan non-functional testing for performance, usability, security evaluation.&quot;&quot;&quot;
        nf_suite = {
            &#x27;suite_purpose&#x27;: &#x27;Evaluate non-functional characteristics for solution assessment&#x27;,
            &#x27;performance_testing&#x27;: self._plan_performance_tests(nf_requirements),
            &#x27;usability_testing&#x27;: self._plan_usability_tests(nf_requirements),
            &#x27;security_testing&#x27;: self._plan_security_tests(nf_requirements),
            &#x27;reliability_testing&#x27;: self._plan_reliability_tests(nf_requirements),
            &#x27;evidence_targets&#x27;: [&#x27;Performance baselines&#x27;, &#x27;User experience metrics&#x27;, &#x27;Security compliance&#x27;]
        }

        return nf_suite

    def _plan_scenario_testing(self, scenarios, criteria):
        &quot;&quot;&quot;Plan realistic scenario testing with different stakeholder perspectives.&quot;&quot;&quot;
        scenario_suite = {
            &#x27;suite_purpose&#x27;: &#x27;Validate solution effectiveness in real-world usage scenarios&#x27;,
            &#x27;stakeholder_scenarios&#x27;: [],
            &#x27;cross_functional_scenarios&#x27;: [],
            &#x27;edge_case_scenarios&#x27;: []
        }

        for scenario in scenarios:
            stakeholder_type = scenario.get(&#x27;stakeholder_type&#x27;, &#x27;generic_user&#x27;)

            scenario_test = {
                &#x27;scenario_name&#x27;: scenario[&#x27;name&#x27;],
                &#x27;stakeholder_type&#x27;: stakeholder_type,
                &#x27;business_context&#x27;: scenario[&#x27;context&#x27;],
                &#x27;workflow_steps&#x27;: scenario[&#x27;steps&#x27;],
                &#x27;success_criteria&#x27;: scenario[&#x27;success_criteria&#x27;],
                &#x27;test_data_requirements&#x27;: self._determine_scenario_test_data(scenario),
                &#x27;evaluation_evidence&#x27;: {
                    &#x27;workflow_completion_rate&#x27;: &#x27;Percentage of successful scenario completions&#x27;,
                    &#x27;user_satisfaction_score&#x27;: &#x27;Stakeholder feedback on scenario experience&#x27;,
                    &#x27;efficiency_metrics&#x27;: &#x27;Time and effort required to complete scenario&#x27;,
                    &#x27;error_handling&#x27;: &#x27;How well solution handles scenario variations&#x27;
                }
            }

            scenario_suite[&#x27;stakeholder_scenarios&#x27;].append(scenario_test)

        return scenario_suite

    def _plan_boundary_testing(self, requirements):
        &quot;&quot;&quot;Plan boundary and edge case testing for robustness evaluation.&quot;&quot;&quot;
        boundary_suite = {
            &#x27;suite_purpose&#x27;: &#x27;Test solution behavior at operational limits and edge cases&#x27;,
            &#x27;data_boundary_tests&#x27;: [],
            &#x27;load_boundary_tests&#x27;: [],
            &#x27;input_validation_tests&#x27;: [],
            &#x27;error_handling_tests&#x27;: []
        }

        # Generate boundary tests based on requirements
        for req in requirements:
            req_description = req.get(&#x27;description&#x27;, &#x27;&#x27;).lower()

            # Data boundary tests
            if &#x27;number&#x27; in req_description or &#x27;amount&#x27; in req_description or &#x27;quantity&#x27; in req_description:
                boundary_suite[&#x27;data_boundary_tests&#x27;].append({
                    &#x27;test_type&#x27;: &#x27;Numeric boundary&#x27;,
                    &#x27;requirement&#x27;: req[&#x27;description&#x27;],
                    &#x27;test_cases&#x27;: [&#x27;Minimum valid value&#x27;, &#x27;Maximum valid value&#x27;, &#x27;Just below minimum&#x27;, &#x27;Just above maximum&#x27;],
                    &#x27;evidence_target&#x27;: &#x27;Data validation robustness&#x27;
                })

            # Input validation tests
            if &#x27;input&#x27; in req_description or &#x27;enter&#x27; in req_description or &#x27;form&#x27; in req_description:
                boundary_suite[&#x27;input_validation_tests&#x27;].append({
                    &#x27;test_type&#x27;: &#x27;Input validation&#x27;,
                    &#x27;requirement&#x27;: req[&#x27;description&#x27;],
                    &#x27;test_cases&#x27;: [&#x27;Empty input&#x27;, &#x27;Maximum length input&#x27;, &#x27;Special characters&#x27;, &#x27;Invalid formats&#x27;],
                    &#x27;evidence_target&#x27;: &#x27;Input handling robustness&#x27;
                })

        return boundary_suite

    def _plan_integration_testing(self):
        &quot;&quot;&quot;Plan integration testing for system-level evaluation.&quot;&quot;&quot;
        integration_suite = {
            &#x27;suite_purpose&#x27;: &#x27;Validate solution integration and system-level behavior&#x27;,
            &#x27;internal_integration&#x27;: &#x27;Testing between solution components&#x27;,
            &#x27;external_integration&#x27;: &#x27;Testing with external systems and services&#x27;,
            &#x27;data_flow_testing&#x27;: &#x27;Validating data consistency across integrations&#x27;,
            &#x27;end_to_end_testing&#x27;: &#x27;Complete workflow testing across all integrations&#x27;
        }

        return integration_suite

    def _plan_evidence_collection(self, criteria):
        &quot;&quot;&quot;Plan systematic evidence collection aligned with evaluation criteria.&quot;&quot;&quot;
        evidence_plan = {
            &#x27;automated_evidence&#x27;: {
                &#x27;test_execution_logs&#x27;: &#x27;Automated collection of test results and metrics&#x27;,
                &#x27;performance_monitoring&#x27;: &#x27;Real-time performance data during testing&#x27;,
                &#x27;error_tracking&#x27;: &#x27;Automated capture of errors and exceptions&#x27;,
                &#x27;coverage_analysis&#x27;: &#x27;Code and requirement coverage measurements&#x27;
            },
            &#x27;manual_evidence&#x27;: {
                &#x27;stakeholder_feedback&#x27;: &#x27;Structured collection of user experience feedback&#x27;,
                &#x27;expert_evaluation&#x27;: &#x27;Technical expert assessment of solution quality&#x27;,
                &#x27;compliance_verification&#x27;: &#x27;Manual verification of regulatory compliance&#x27;,
                &#x27;usability_observations&#x27;: &#x27;Observed user behavior during scenario testing&#x27;
            },
            &#x27;evidence_mapping&#x27;: self._map_evidence_to_criteria(criteria),
            &#x27;quality_assurance&#x27;: {
                &#x27;evidence_validation&#x27;: &#x27;Verification of evidence accuracy and completeness&#x27;,
                &#x27;bias_mitigation&#x27;: &#x27;Procedures to reduce evaluation bias&#x27;,
                &#x27;documentation_standards&#x27;: &#x27;Consistent evidence documentation formats&#x27;
            }
        }

        return evidence_plan

    def _generate_functional_test_cases(self, requirement):
        &quot;&quot;&quot;Generate specific test cases for a functional requirement.&quot;&quot;&quot;
        test_cases = []

        req_description = requirement.get(&#x27;description&#x27;, &#x27;&#x27;).lower()

        # Basic positive test case
        test_cases.append({
            &#x27;case_id&#x27;: f&quot;{requirement.get(&#x27;id&#x27;, &#x27;req&#x27;)}_positive&quot;,
            &#x27;description&#x27;: f&quot;Verify {requirement[&#x27;description&#x27;]} works correctly with valid inputs&quot;,
            &#x27;test_type&#x27;: &#x27;positive&#x27;,
            &#x27;expected_outcome&#x27;: &#x27;Requirement satisfied successfully&#x27;
        })

        # Negative test case
        test_cases.append({
            &#x27;case_id&#x27;: f&quot;{requirement.get(&#x27;id&#x27;, &#x27;req&#x27;)}_negative&quot;,
            &#x27;description&#x27;: f&quot;Verify {requirement[&#x27;description&#x27;]} handles invalid inputs appropriately&quot;,
            &#x27;test_type&#x27;: &#x27;negative&#x27;,
            &#x27;expected_outcome&#x27;: &#x27;Appropriate error handling or validation&#x27;
        })

        # Edge case if applicable
        if &#x27;calculate&#x27; in req_description or &#x27;process&#x27; in req_description:
            test_cases.append({
                &#x27;case_id&#x27;: f&quot;{requirement.get(&#x27;id&#x27;, &#x27;req&#x27;)}_edge&quot;,
                &#x27;description&#x27;: f&quot;Verify {requirement[&#x27;description&#x27;]} handles edge cases correctly&quot;,
                &#x27;test_type&#x27;: &#x27;edge_case&#x27;,
                &#x27;expected_outcome&#x27;: &#x27;Correct behavior at operational boundaries&#x27;
            })

        return test_cases

    def _plan_performance_tests(self, nf_requirements):
        &quot;&quot;&quot;Plan performance testing for non-functional evaluation.&quot;&quot;&quot;
        performance_tests = []

        for req in nf_requirements:
            if &#x27;performance&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower() or &#x27;response&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower():
                performance_tests.append({
                    &#x27;test_name&#x27;: f&quot;Performance validation for {req[&#x27;description&#x27;]}&quot;,
                    &#x27;test_type&#x27;: &#x27;load_testing&#x27;,
                    &#x27;metrics&#x27;: [&#x27;Response time&#x27;, &#x27;Throughput&#x27;, &#x27;Resource utilization&#x27;],
                    &#x27;success_criteria&#x27;: req.get(&#x27;target_value&#x27;, &#x27;Under 3 seconds response time&#x27;)
                })

        return performance_tests

    def _plan_usability_tests(self, nf_requirements):
        &quot;&quot;&quot;Plan usability testing for user experience evaluation.&quot;&quot;&quot;
        usability_tests = []

        for req in nf_requirements:
            if &#x27;usability&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower() or &#x27;user&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower():
                usability_tests.append({
                    &#x27;test_name&#x27;: f&quot;Usability validation for {req[&#x27;description&#x27;]}&quot;,
                    &#x27;test_type&#x27;: &#x27;user_testing&#x27;,
                    &#x27;metrics&#x27;: [&#x27;Task completion rate&#x27;, &#x27;User satisfaction&#x27;, &#x27;Error rate&#x27;],
                    &#x27;participants&#x27;: &#x27;Representative stakeholder groups&#x27;
                })

        return usability_tests

    def _plan_security_tests(self, nf_requirements):
        &quot;&quot;&quot;Plan security testing for security requirement evaluation.&quot;&quot;&quot;
        security_tests = []

        for req in nf_requirements:
            if &#x27;security&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower() or &#x27;access&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower():
                security_tests.append({
                    &#x27;test_name&#x27;: f&quot;Security validation for {req[&#x27;description&#x27;]}&quot;,
                    &#x27;test_type&#x27;: &#x27;security_testing&#x27;,
                    &#x27;areas&#x27;: [&#x27;Authentication&#x27;, &#x27;Authorization&#x27;, &#x27;Data protection&#x27;, &#x27;Input validation&#x27;],
                    &#x27;compliance_check&#x27;: &#x27;Verify against security standards&#x27;
                })

        return security_tests

    def _plan_reliability_tests(self, nf_requirements):
        &quot;&quot;&quot;Plan reliability testing for system stability evaluation.&quot;&quot;&quot;
        reliability_tests = []

        for req in nf_requirements:
            if &#x27;reliability&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower() or &#x27;availability&#x27; in req.get(&#x27;description&#x27;, &#x27;&#x27;).lower():
                reliability_tests.append({
                    &#x27;test_name&#x27;: f&quot;Reliability validation for {req[&#x27;description&#x27;]}&quot;,
                    &#x27;test_type&#x27;: &#x27;reliability_testing&#x27;,
                    &#x27;duration&#x27;: &#x27;Extended testing period&#x27;,
                    &#x27;metrics&#x27;: [&#x27;Uptime&#x27;, &#x27;Error recovery&#x27;, &#x27;Data consistency&#x27;]
                })

        return reliability_tests

    def _determine_scenario_test_data(self, scenario):
        &quot;&quot;&quot;Determine test data requirements for scenario testing.&quot;&quot;&quot;
        test_data = {
            &#x27;data_volume&#x27;: &#x27;Realistic data volumes matching production usage&#x27;,
            &#x27;data_variety&#x27;: &#x27;Representative data covering different scenario variations&#x27;,
            &#x27;data_quality&#x27;: &#x27;High-quality data that enables realistic testing&#x27;,
            &#x27;data_privacy&#x27;: &#x27;Anonymized or synthetic data protecting real user information&#x27;
        }

        # Customize based on scenario type
        scenario_context = scenario.get(&#x27;context&#x27;, &#x27;&#x27;).lower()
        if &#x27;student&#x27; in scenario_context:
            test_data[&#x27;specific_requirements&#x27;] = &#x27;Student records, grades, enrollment data&#x27;
        elif &#x27;financial&#x27; in scenario_context:
            test_data[&#x27;specific_requirements&#x27;] = &#x27;Transaction records, account data, payment information&#x27;
        elif &#x27;inventory&#x27; in scenario_context:
            test_data[&#x27;specific_requirements&#x27;] = &#x27;Product data, stock levels, supplier information&#x27;

        return test_data

    def _map_requirement_to_criteria(self, requirement, criteria):
        &quot;&quot;&quot;Map functional requirements to evaluation criteria.&quot;&quot;&quot;
        mapping = []

        req_desc = requirement.get(&#x27;description&#x27;, &#x27;&#x27;).lower()

        for criterion in criteria:
            criterion_desc = criterion.get(&#x27;description&#x27;, &#x27;&#x27;).lower()

            # Simple keyword matching - in practice would be more sophisticated
            if any(word in criterion_desc for word in req_desc.split() if len(word) &gt; 3):
                mapping.append({
                    &#x27;criterion_id&#x27;: criterion.get(&#x27;id&#x27;),
                    &#x27;criterion_description&#x27;: criterion.get(&#x27;description&#x27;),
                    &#x27;evidence_contribution&#x27;: &#x27;Functional validation supports criterion assessment&#x27;
                })

        return mapping

    def _map_evidence_to_criteria(self, criteria):
        &quot;&quot;&quot;Map evidence collection to specific evaluation criteria.&quot;&quot;&quot;
        mapping = {}

        for criterion in criteria:
            criterion_id = criterion.get(&#x27;id&#x27;, &#x27;unknown&#x27;)
            criterion_desc = criterion.get(&#x27;description&#x27;, &#x27;&#x27;).lower()

            if &#x27;performance&#x27; in criterion_desc:
                mapping[criterion_id] = [&#x27;Performance test results&#x27;, &#x27;Response time measurements&#x27;, &#x27;Load test outcomes&#x27;]
            elif &#x27;usability&#x27; in criterion_desc:
                mapping[criterion_id] = [&#x27;User testing feedback&#x27;, &#x27;Task completion rates&#x27;, &#x27;Satisfaction surveys&#x27;]
            elif &#x27;security&#x27; in criterion_desc:
                mapping[criterion_id] = [&#x27;Security test results&#x27;, &#x27;Vulnerability assessments&#x27;, &#x27;Compliance verification&#x27;]
            elif &#x27;functional&#x27; in criterion_desc:
                mapping[criterion_id] = [&#x27;Test case execution results&#x27;, &#x27;Requirement coverage&#x27;, &#x27;Defect analysis&#x27;]
            else:
                mapping[criterion_id] = [&#x27;General test results&#x27;, &#x27;Stakeholder feedback&#x27;, &#x27;Expert evaluation&#x27;]

        return mapping

    def execute_test_plan_for_evaluation(self, test_execution_data, stakeholder_feedback_data, 
                                       performance_metrics_data):
        &quot;&quot;&quot;
        Execute test plan and collect evaluation evidence.

        Args:
            test_execution_data: Results from automated and manual test execution
            stakeholder_feedback_data: Feedback collected during scenario testing
            performance_metrics_data: Performance measurements from testing
        &quot;&quot;&quot;
        execution_results = {
            &#x27;execution_summary&#x27;: {
                &#x27;total_test_cases&#x27;: test_execution_data.get(&#x27;total_cases&#x27;, 0),
                &#x27;executed_cases&#x27;: test_execution_data.get(&#x27;executed_cases&#x27;, 0),
                &#x27;passed_cases&#x27;: test_execution_data.get(&#x27;passed_cases&#x27;, 0),
                &#x27;failed_cases&#x27;: test_execution_data.get(&#x27;failed_cases&#x27;, 0),
                &#x27;blocked_cases&#x27;: test_execution_data.get(&#x27;blocked_cases&#x27;, 0),
                &#x27;execution_coverage&#x27;: self._calculate_execution_coverage(test_execution_data)
            },
            &#x27;functional_results&#x27;: self._analyze_functional_test_results(test_execution_data),
            &#x27;non_functional_results&#x27;: self._analyze_non_functional_results(performance_metrics_data),
            &#x27;scenario_results&#x27;: self._analyze_scenario_test_results(stakeholder_feedback_data),
            &#x27;evaluation_evidence&#x27;: self._extract_evaluation_evidence(
                test_execution_data, stakeholder_feedback_data, performance_metrics_data
            )
        }

        self.testing_outcomes = execution_results
        return execution_results

    def _calculate_execution_coverage(self, execution_data):
        &quot;&quot;&quot;Calculate test execution coverage for evaluation purposes.&quot;&quot;&quot;
        total_cases = execution_data.get(&#x27;total_cases&#x27;, 1)
        executed_cases = execution_data.get(&#x27;executed_cases&#x27;, 0)

        coverage = (executed_cases / total_cases) * 100 if total_cases &gt; 0 else 0

        return {
            &#x27;percentage&#x27;: coverage,
            &#x27;assessment&#x27;: &#x27;Excellent&#x27; if coverage &gt;= 95 else &#x27;Good&#x27; if coverage &gt;= 85 else &#x27;Needs Improvement&#x27;
        }

    def _analyze_functional_test_results(self, execution_data):
        &quot;&quot;&quot;Analyze functional test results for evaluation evidence.&quot;&quot;&quot;
        functional_results = {
            &#x27;requirement_coverage&#x27;: {},
            &#x27;defect_analysis&#x27;: {},
            &#x27;compliance_assessment&#x27;: {}
        }

        # Analyze by test category
        for category in execution_data.get(&#x27;categories&#x27;, []):
            category_name = category.get(&#x27;name&#x27;, &#x27;unknown&#x27;)
            category_results = category.get(&#x27;results&#x27;, {})

            functional_results[&#x27;requirement_coverage&#x27;][category_name] = {
                &#x27;tests_executed&#x27;: category_results.get(&#x27;executed&#x27;, 0),
                &#x27;tests_passed&#x27;: category_results.get(&#x27;passed&#x27;, 0),
                &#x27;compliance_percentage&#x27;: (category_results.get(&#x27;passed&#x27;, 0) / max(category_results.get(&#x27;executed&#x27;, 1), 1)) * 100
            }

        return functional_results

    def _analyze_non_functional_results(self, performance_data):
        &quot;&quot;&quot;Analyze non-functional test results for evaluation evidence.&quot;&quot;&quot;
        nf_results = {
            &#x27;performance_assessment&#x27;: {},
            &#x27;scalability_results&#x27;: {},
            &#x27;reliability_metrics&#x27;: {}
        }

        if performance_data:
            nf_results[&#x27;performance_assessment&#x27;] = {
                &#x27;average_response_time&#x27;: performance_data.get(&#x27;avg_response_time&#x27;, 0),
                &#x27;throughput&#x27;: performance_data.get(&#x27;throughput&#x27;, 0),
                &#x27;resource_utilization&#x27;: performance_data.get(&#x27;resource_usage&#x27;, {}),
                &#x27;performance_rating&#x27;: self._rate_performance(performance_data)
            }

        return nf_results

    def _analyze_scenario_test_results(self, feedback_data):
        &quot;&quot;&quot;Analyze scenario testing results for stakeholder evaluation evidence.&quot;&quot;&quot;
        scenario_results = {
            &#x27;workflow_success_rates&#x27;: {},
            &#x27;stakeholder_satisfaction&#x27;: {},
            &#x27;usability_metrics&#x27;: {}
        }

        if feedback_data:
            scenario_results[&#x27;stakeholder_satisfaction&#x27;] = {
                &#x27;average_rating&#x27;: self._calculate_average_rating(feedback_data),
                &#x27;satisfaction_distribution&#x27;: self._analyze_rating_distribution(feedback_data),
                &#x27;key_feedback_themes&#x27;: self._extract_feedback_themes(feedback_data)
            }

        return scenario_results

    def _extract_evaluation_evidence(self, execution_data, feedback_data, performance_data):
        &quot;&quot;&quot;Extract specific evidence for evaluation criteria assessment.&quot;&quot;&quot;
        evidence = {
            &#x27;functional_evidence&#x27;: {
                &#x27;requirement_compliance_rate&#x27;: self._calculate_compliance_rate(execution_data),
                &#x27;defect_density&#x27;: self._calculate_defect_density(execution_data),
                &#x27;feature_completeness&#x27;: self._assess_feature_completeness(execution_data)
            },
            &#x27;quality_evidence&#x27;: {
                &#x27;test_coverage_achieved&#x27;: self._calculate_test_coverage(execution_data),
                &#x27;performance_benchmarks&#x27;: self._extract_performance_benchmarks(performance_data),
                &#x27;user_acceptance_level&#x27;: self._assess_user_acceptance(feedback_data)
            },
            &#x27;stakeholder_evidence&#x27;: {
                &#x27;satisfaction_scores&#x27;: self._extract_satisfaction_scores(feedback_data),
                &#x27;workflow_efficiency&#x27;: self._measure_workflow_efficiency(feedback_data),
                &#x27;adoption_indicators&#x27;: self._assess_adoption_potential(feedback_data)
            }
        }

        return evidence

    def _rate_performance(self, performance_data):
        &quot;&quot;&quot;Rate overall performance based on test results.&quot;&quot;&quot;
        avg_response = performance_data.get(&#x27;avg_response_time&#x27;, 5)

        if avg_response &lt; 1:
            return &#x27;Excellent&#x27;
        elif avg_response &lt; 3:
            return &#x27;Good&#x27;
        elif avg_response &lt; 5:
            return &#x27;Acceptable&#x27;
        else:
            return &#x27;Needs Improvement&#x27;

    def _calculate_average_rating(self, feedback_data):
        &quot;&quot;&quot;Calculate average satisfaction rating from feedback.&quot;&quot;&quot;
        if not feedback_data:
            return 0

        ratings = [item.get(&#x27;satisfaction_rating&#x27;, 3) for item in feedback_data if &#x27;satisfaction_rating&#x27; in item]
        return sum(ratings) / len(ratings) if ratings else 0

    def _analyze_rating_distribution(self, feedback_data):
        &quot;&quot;&quot;Analyze distribution of satisfaction ratings.&quot;&quot;&quot;
        distribution = {&#x27;5&#x27;: 0, &#x27;4&#x27;: 0, &#x27;3&#x27;: 0, &#x27;2&#x27;: 0, &#x27;1&#x27;: 0}

        for item in feedback_data:
            rating = str(int(item.get(&#x27;satisfaction_rating&#x27;, 3)))
            if rating in distribution:
                distribution[rating] += 1

        return distribution

    def _extract_feedback_themes(self, feedback_data):
        &quot;&quot;&quot;Extract common themes from qualitative feedback.&quot;&quot;&quot;
        # Simplified theme extraction
        themes = []
        all_feedback = &#x27; &#x27;.join([item.get(&#x27;comments&#x27;, &#x27;&#x27;) for item in feedback_data]).lower()

        theme_keywords = {
            &#x27;ease_of_use&#x27;: [&#x27;easy&#x27;, &#x27;simple&#x27;, &#x27;intuitive&#x27;],
            &#x27;performance&#x27;: [&#x27;fast&#x27;, &#x27;slow&#x27;, &#x27;responsive&#x27;],
            &#x27;functionality&#x27;: [&#x27;features&#x27;, &#x27;works&#x27;, &#x27;useful&#x27;],
            &#x27;issues&#x27;: [&#x27;problem&#x27;, &#x27;difficult&#x27;, &#x27;confusing&#x27;]
        }

        for theme, keywords in theme_keywords.items():
            if any(keyword in all_feedback for keyword in keywords):
                themes.append(theme)

        return themes

    def generate_testing_evaluation_report(self):
        &quot;&quot;&quot;Generate comprehensive report linking testing outcomes to evaluation.&quot;&quot;&quot;
        if not self.testing_outcomes:
            return &quot;No testing outcomes available for evaluation reporting.&quot;

        report = f&quot;&quot;&quot;
TESTING-BASED EVALUATION REPORT
Solution: {self.solution_name}
Evaluation Objectives: {&#x27;, &#x27;.join(self.evaluation_objectives)}
Report Date: 2025-09-20

TEST EXECUTION SUMMARY:
{&#x27;-&#x27; * 30}
&quot;&quot;&quot;

        execution_summary = self.testing_outcomes.get(&#x27;execution_summary&#x27;, {})
        report += f&quot;Total Test Cases: {execution_summary.get(&#x27;total_test_cases&#x27;, 0)}\n&quot;
        report += f&quot;Executed: {execution_summary.get(&#x27;executed_cases&#x27;, 0)}\n&quot;
        report += f&quot;Passed: {execution_summary.get(&#x27;passed_cases&#x27;, 0)}\n&quot;
        report += f&quot;Failed: {execution_summary.get(&#x27;failed_cases&#x27;, 0)}\n&quot;

        coverage = execution_summary.get(&#x27;execution_coverage&#x27;, {})
        report += f&quot;Execution Coverage: {coverage.get(&#x27;percentage&#x27;, 0):.1f}% ({coverage.get(&#x27;assessment&#x27;, &#x27;Unknown&#x27;)})\n&quot;

        # Evaluation evidence summary
        evidence = self.testing_outcomes.get(&#x27;evaluation_evidence&#x27;, {})

        report += f&quot;\nEVALUATION EVIDENCE FROM TESTING:\n&quot;
        report += f&quot;{&#x27;-&#x27; * 40}\n&quot;

        functional_evidence = evidence.get(&#x27;functional_evidence&#x27;, {})
        report += f&quot;Functional Compliance: {functional_evidence.get(&#x27;requirement_compliance_rate&#x27;, 0):.1f}%\n&quot;
        report += f&quot;Defect Density: {functional_evidence.get(&#x27;defect_density&#x27;, 0):.2f} defects/KLOC\n&quot;

        quality_evidence = evidence.get(&#x27;quality_evidence&#x27;, {})
        report += f&quot;Test Coverage: {quality_evidence.get(&#x27;test_coverage_achieved&#x27;, 0):.1f}%\n&quot;
        report += f&quot;User Acceptance: {quality_evidence.get(&#x27;user_acceptance_level&#x27;, &#x27;Not assessed&#x27;)}\n&quot;

        stakeholder_evidence = evidence.get(&#x27;stakeholder_evidence&#x27;, {})
        satisfaction_scores = stakeholder_evidence.get(&#x27;satisfaction_scores&#x27;, {})
        report += f&quot;Average Satisfaction: {satisfaction_scores.get(&#x27;overall_average&#x27;, 0):.1f}/5.0\n&quot;

        return report

def demonstrate_evaluation_test_planning():
    &quot;&quot;&quot;
    Practical example of creating evaluation-focused test plans for a library system.
    &quot;&quot;&quot;
    print(&quot;EVALUATION TEST PLANNING EXAMPLE&quot;)
    print(&quot;=&quot; * 34)

    # Create evaluation test planner
    planner = EvaluationTestPlanner(
        &quot;Digital Library Management System&quot;,
        [&quot;Assess functional completeness&quot;, &quot;Evaluate user satisfaction&quot;, &quot;Validate performance requirements&quot;]
    )

    # Define requirements for testing
    functional_requirements = [
        {&#x27;id&#x27;: &#x27;F001&#x27;, &#x27;description&#x27;: &#x27;System allows users to search for books by title, author, or ISBN&#x27;},
        {&#x27;id&#x27;: &#x27;F002&#x27;, &#x27;description&#x27;: &#x27;System enables users to place holds on available books&#x27;},
        {&#x27;id&#x27;: &#x27;F003&#x27;, &#x27;description&#x27;: &#x27;System processes book checkouts and returns accurately&#x27;},
        {&#x27;id&#x27;: &#x27;F004&#x27;, &#x27;description&#x27;: &#x27;System generates overdue notices automatically&#x27;},
        {&#x27;id&#x27;: &#x27;F005&#x27;, &#x27;description&#x27;: &#x27;System maintains accurate inventory tracking&#x27;}
    ]

    non_functional_requirements = [
        {&#x27;id&#x27;: &#x27;NF001&#x27;, &#x27;description&#x27;: &#x27;System responds to search queries within 2 seconds&#x27;},
        {&#x27;id&#x27;: &#x27;NF002&#x27;, &#x27;description&#x27;: &#x27;System supports 200 concurrent users without performance degradation&#x27;},
        {&#x27;id&#x27;: &#x27;NF003&#x27;, &#x27;description&#x27;: &#x27;System interface is usable by people with visual impairments&#x27;},
        {&#x27;id&#x27;: &#x27;NF004&#x27;, &#x27;description&#x27;: &#x27;System maintains 99% uptime during library operating hours&#x27;},
        {&#x27;id&#x27;: &#x27;NF005&#x27;, &#x27;description&#x27;: &#x27;System protects patron privacy and data security&#x27;}
    ]

    stakeholder_scenarios = [
        {
            &#x27;name&#x27;: &#x27;Student Research Workflow&#x27;,
            &#x27;stakeholder_type&#x27;: &#x27;student&#x27;,
            &#x27;context&#x27;: &#x27;Student researching for academic paper&#x27;,
            &#x27;steps&#x27;: [&#x27;Search for academic sources&#x27;, &#x27;Place holds on relevant books&#x27;, &#x27;Check availability&#x27;, &#x27;Access digital resources&#x27;],
            &#x27;success_criteria&#x27;: &#x27;Complete research material gathering within 15 minutes&#x27;
        },
        {
            &#x27;name&#x27;: &#x27;Librarian Daily Operations&#x27;,
            &#x27;stakeholder_type&#x27;: &#x27;librarian&#x27;,
            &#x27;context&#x27;: &#x27;Librarian managing daily circulation tasks&#x27;,
            &#x27;steps&#x27;: [&#x27;Process returns&#x27;, &#x27;Handle hold requests&#x27;, &#x27;Assist patron searches&#x27;, &#x27;Generate reports&#x27;],
            &#x27;success_criteria&#x27;: &#x27;Complete routine tasks 30% faster than previous system&#x27;
        },
        {
            &#x27;name&#x27;: &#x27;Community Member Casual Use&#x27;,
            &#x27;stakeholder_type&#x27;: &#x27;community_member&#x27;,
            &#x27;context&#x27;: &#x27;Community member looking for recreational reading&#x27;,
            &#x27;steps&#x27;: [&#x27;Browse new arrivals&#x27;, &#x27;Search by genre&#x27;, &#x27;Check out books&#x27;, &#x27;Renew online&#x27;],
            &#x27;success_criteria&#x27;: &#x27;Intuitive experience requiring no staff assistance&#x27;
        }
    ]

    evaluation_criteria = [
        {&#x27;id&#x27;: &#x27;EC001&#x27;, &#x27;description&#x27;: &#x27;Functional requirements compliance above 95%&#x27;},
        {&#x27;id&#x27;: &#x27;EC002&#x27;, &#x27;description&#x27;: &#x27;User satisfaction scores above 4.0/5.0&#x27;},
        {&#x27;id&#x27;: &#x27;EC003&#x27;, &#x27;description&#x27;: &#x27;Performance benchmarks met consistently&#x27;},
        {&#x27;id&#x27;: &#x27;EC004&#x27;, &#x27;description&#x27;: &#x27;Accessibility standards compliance verified&#x27;},
        {&#x27;id&#x27;: &#x27;EC005&#x27;, &#x27;description&#x27;: &#x27;System reliability demonstrated through testing&#x27;}
    ]

    # Create comprehensive test plan
    test_plan = planner.create_evaluation_test_plan(
        functional_requirements, 
        non_functional_requirements,
        stakeholder_scenarios,
        evaluation_criteria
    )

    # Execute test plan (simulated results)
    test_execution_data = {
        &#x27;total_cases&#x27;: 87,
        &#x27;executed_cases&#x27;: 85,
        &#x27;passed_cases&#x27;: 78,
        &#x27;failed_cases&#x27;: 7,
        &#x27;blocked_cases&#x27;: 2,
        &#x27;categories&#x27;: [
            {&#x27;name&#x27;: &#x27;Search Functionality&#x27;, &#x27;results&#x27;: {&#x27;executed&#x27;: 25, &#x27;passed&#x27;: 23, &#x27;failed&#x27;: 2}},
            {&#x27;name&#x27;: &#x27;Hold Management&#x27;, &#x27;results&#x27;: {&#x27;executed&#x27;: 18, &#x27;passed&#x27;: 17, &#x27;failed&#x27;: 1}},
            {&#x27;name&#x27;: &#x27;Checkout Process&#x27;, &#x27;results&#x27;: {&#x27;executed&#x27;: 20, &#x27;passed&#x27;: 19, &#x27;failed&#x27;: 1}},
            {&#x27;name&#x27;: &#x27;Reporting&#x27;, &#x27;results&#x27;: {&#x27;executed&#x27;: 15, &#x27;passed&#x27;: 13, &#x27;failed&#x27;: 2}},
            {&#x27;name&#x27;: &#x27;Integration&#x27;, &#x27;results&#x27;: {&#x27;executed&#x27;: 7, &#x27;passed&#x27;: 6, &#x27;failed&#x27;: 1}}
        ]
    }

    stakeholder_feedback_data = [
        {&#x27;stakeholder_type&#x27;: &#x27;student&#x27;, &#x27;satisfaction_rating&#x27;: 4.2, &#x27;comments&#x27;: &#x27;Much easier to find academic sources&#x27;},
        {&#x27;stakeholder_type&#x27;: &#x27;student&#x27;, &#x27;satisfaction_rating&#x27;: 3.8, &#x27;comments&#x27;: &#x27;Search could be more intuitive&#x27;},
        {&#x27;stakeholder_type&#x27;: &#x27;librarian&#x27;, &#x27;satisfaction_rating&#x27;: 4.5, &#x27;comments&#x27;: &#x27;Significant improvement in efficiency&#x27;},
        {&#x27;stakeholder_type&#x27;: &#x27;librarian&#x27;, &#x27;satisfaction_rating&#x27;: 4.1, &#x27;comments&#x27;: &#x27;Reports are much more useful now&#x27;},
        {&#x27;stakeholder_type&#x27;: &#x27;community_member&#x27;, &#x27;satisfaction_rating&#x27;: 4.3, &#x27;comments&#x27;: &#x27;Love the new interface&#x27;},
        {&#x27;stakeholder_type&#x27;: &#x27;community_member&#x27;, &#x27;satisfaction_rating&#x27;: 3.9, &#x27;comments&#x27;: &#x27;Online renewals work great&#x27;}
    ]

    performance_metrics_data = {
        &#x27;avg_response_time&#x27;: 1.3,
        &#x27;throughput&#x27;: 180,
        &#x27;resource_usage&#x27;: {&#x27;cpu&#x27;: 45, &#x27;memory&#x27;: 62, &#x27;database&#x27;: 38},
        &#x27;concurrent_users_tested&#x27;: 250,
        &#x27;uptime_percentage&#x27;: 99.2
    }

    # Execute test plan and analyze results
    testing_outcomes = planner.execute_test_plan_for_evaluation(
        test_execution_data, stakeholder_feedback_data, performance_metrics_data
    )

    # Generate evaluation report
    evaluation_report = planner.generate_testing_evaluation_report()
    print(evaluation_report)

    print(f&quot;\nTEST PLAN SUMMARY:&quot;)
    print(f&quot;Functional Test Categories: {len(test_plan[&#x27;functional_test_suite&#x27;][&#x27;test_categories&#x27;])}&quot;)
    print(f&quot;Stakeholder Scenarios: {len(test_plan[&#x27;scenario_test_suite&#x27;][&#x27;stakeholder_scenarios&#x27;])}&quot;)
    print(f&quot;Evidence Collection Methods: {len(test_plan[&#x27;evidence_collection_plan&#x27;][&#x27;automated_evidence&#x27;])}&quot;)

    print(f&quot;\nEVALUATION INSIGHTS:&quot;)
    execution_summary = testing_outcomes[&#x27;execution_summary&#x27;]
    print(f&quot;Test Pass Rate: {(execution_summary[&#x27;passed_cases&#x27;]/execution_summary[&#x27;executed_cases&#x27;]*100):.1f}%&quot;)

    evidence = testing_outcomes[&#x27;evaluation_evidence&#x27;]
    functional_evidence = evidence.get(&#x27;functional_evidence&#x27;, {})
    print(f&quot;Functional Compliance: {functional_evidence.get(&#x27;requirement_compliance_rate&#x27;, &#x27;Not calculated&#x27;)}&quot;)

    return planner

# Run demonstration
if __name__ == &quot;__main__&quot;:
    test_planning_demo = demonstrate_evaluation_test_planning()
</code></pre>
</div>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 15, 2025 07:03:58 UTC">October 15, 2025</span>
  </span>

    
    
    
    
  </aside>


  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by creating an <a href="https://github.com/Eatham532/Software-Engineering-HSC-Textbook/issues/new/choose" target="_blank" rel="noopener">issue</a> on github.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        

<!-- Footer -->
<footer class="md-footer">
  
    <!-- Custom footer with left/right layout -->
    <div class="md-footer__inner md-grid">
      <!-- Left side: Copyright and cookie settings -->
      <div class="md-footer__left">
        <div class="md-footer__copyright">
          
            Copyright &copy; 2025 Eatham532 ‚Äì <a href="#__consent">Change cookie settings</a>

          
        </div>
      </div>

      <!-- Right side: Page name and BETA -->
      <div class="md-footer__right">
        <div class="md-footer__page-info">
          <span class="md-footer__page-title">Content</span>
          
            <span class="md-footer__beta-badge">BETA</span>
          
        </div>
      </div>
    </div>
  
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
      <div class="md-consent" data-md-component="consent" id="__consent" hidden>
        <div class="md-consent__overlay"></div>
        <aside class="md-consent__inner">
          <form class="md-consent__form md-grid md-typeset" name="consent">
            



<h4>Cookie consent</h4>
<p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of the textbook. With your consent, you're helping us to make software engineering education better for everyone.</p>
<input class="md-toggle" type="checkbox" id="__settings" >
<div class="md-consent__settings">
  <ul class="task-list">
    
    
    
      
        
  
  
    
    
  
  <li class="task-list-item">
    <label class="task-list-control">
      <input type="checkbox" name="github" checked>
      <span class="task-list-indicator"></span>
      GitHub
    </label>
  </li>

      
    
    
      
        
  
  
    
    
  
  <li class="task-list-item">
    <label class="task-list-control">
      <input type="checkbox" name="analytics" checked>
      <span class="task-list-indicator"></span>
      Google Analytics
    </label>
  </li>

      
    
  </ul>
</div>
<div class="md-consent__controls">
  
    
      <button class="md-button md-button--primary">Accept</button>
    
    
    
  
    
    
    
      <label class="md-button" for="__settings">Manage settings</label>
    
  
</div>
          </form>
        </aside>
      </div>
      <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script>
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tabs", "navigation.top", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "search.highlight", "search.suggest", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.action.edit", "content.action.view", "toc.follow"], "search": "../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../../../assets/diagram-modal.js"></script>
      
        <script src="../../../../assets/quiz.js"></script>
      
        <script src="../../../../assets/code-blocks.js"></script>
      
    
  </body>
</html>